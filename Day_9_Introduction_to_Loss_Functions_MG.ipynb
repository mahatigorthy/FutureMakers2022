{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day_9_Introduction_to_Loss_Functions_MG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahatigorthy/FutureMakers2022/blob/main/Day_9_Introduction_to_Loss_Functions_MG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "861ncVuLPeyF"
      },
      "source": [
        "![image_2021-10-30_133041.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 9 Objectives: \n",
        "* To introduce you to loss functions. \n"
      ],
      "metadata": {
        "id": "5rokz_Xr0kDG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCtJpzBrYWqr"
      },
      "source": [
        "# Loss Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4fAWIkUYUyR"
      },
      "source": [
        "Loss functions define what a good prediction is and isn’t. Choosing the right loss function dictates how well your estimator (machine learning model) will be. The criteria by which an estimator is scrutinized is its performance - how accurate the model's decisions are. This calls for a way to measure how far a particular iteration of the model is from the actual values. This is where loss functions come into play.\n",
        "\n",
        "Loss functions measure how far an estimated value is from its true value. A loss function maps decisions to their associated costs. Loss functions are not fixed, they change depending on the task in hand and the goal to be met.\n",
        "\n",
        "Worth to note we can speak of different kind of loss functions: **regression loss** functions and **classification loss** functions.\n",
        "\n",
        "Regression loss function describes the difference between the values that a model is predicting and the actual values of the labels. So the loss function has a meaning on a labeled data when we compare the prediction to the label at a single point of time. This loss function is often called the error function or the error formula. Typical error functions we use for regression models are L1 and L2, Huber loss, Quantile loss, log cosh loss.\n",
        "\n",
        "**Note**: L1 loss is also know as Mean Absolute Error. L2 Loss is also know as Mean Square Error or Quadratic loss.\n",
        "\n",
        "Loss functions for classification represent the price paid for inaccuracy of predictions in classification problems (problems of identifying which category a particular observation belongs to). To name a few: log loss, focal loss, exponential loss, hinge loss, relative entropy loss and other.\n",
        "\n",
        "*Note*: While more commonly used in regression, the square loss function can be re-written and utilized for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I1Y9BQxq72l"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6Hpicvr6XJ0"
      },
      "source": [
        "# Regression Losses\n",
        "\n",
        "Remember, in regression, the output would be a real value. We need some loss functions which compares two real values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMoFFw2VUR7j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6982342-7eb4-4677-82ef-348cccb0035d"
      },
      "source": [
        "(train_features, train_labels), (test_features, test_labels) = keras.datasets.boston_housing.load_data()\n",
        "\n",
        "# get per-feature statistics (mean, standard deviation) from the training set to normalize by\n",
        "train_mean = np.mean(train_features, axis=0)\n",
        "train_std = np.std(train_features, axis=0)\n",
        "train_features = (train_features - train_mean) / train_std"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n",
            "65536/57026 [==================================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAdSFpdB6aDH"
      },
      "source": [
        "## Mean Squared Error [MSE]\n",
        "\n",
        "As the name suggests, Mean square error is measured as the average of squared difference between predictions and actual observations. It’s only concerned with the average magnitude of error irrespective of their direction. \n",
        "\n",
        "However, due to squaring, predictions which are far away from actual values are penalized heavily in comparison to less deviated predictions. Plus MSE has nice mathematical properties which makes it easier to calculate gradients.\n",
        "\n",
        "Let's assume there are $n$ data samples, for $i^{th}$ sample; the actual output is $y_i$ and $\\hat{y}_i$ is the estimated output from the regression model. \n",
        "\n",
        "We first square the difference between the original and estimated output with $(y_i - \\hat{y}_i)^2$. Then we take sum of the squared difference for all the samples. And finally divide it by the total count of samples, which is $n$. \n",
        "\n",
        "$$MSE = \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5OO5ZfoYtCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e126be6-4347-406f-a506-9782e3e8a904"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(10, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='mse',\n",
        "              metrics=['mse'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 4s 16ms/step - loss: 577.9760 - mse: 577.9760 - val_loss: 478.2440 - val_mse: 478.2440\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 548.9362 - mse: 548.9362 - val_loss: 448.3910 - val_mse: 448.3910\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 510.7210 - mse: 510.7210 - val_loss: 407.0437 - val_mse: 407.0437\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 457.6028 - mse: 457.6028 - val_loss: 346.4677 - val_mse: 346.4677\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 379.4414 - mse: 379.4414 - val_loss: 262.9777 - val_mse: 262.9777\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 276.5009 - mse: 276.5009 - val_loss: 164.3657 - val_mse: 164.3657\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 168.1907 - mse: 168.1907 - val_loss: 80.4739 - val_mse: 80.4739\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 90.0618 - mse: 90.0618 - val_loss: 48.6865 - val_mse: 48.6865\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 65.3291 - mse: 65.3291 - val_loss: 43.9586 - val_mse: 43.9586\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 55.1099 - mse: 55.1099 - val_loss: 34.2239 - val_mse: 34.2239\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 46.5059 - mse: 46.5059 - val_loss: 28.6188 - val_mse: 28.6188\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 41.5788 - mse: 41.5788 - val_loss: 25.5497 - val_mse: 25.5497\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 38.2656 - mse: 38.2656 - val_loss: 23.9528 - val_mse: 23.9528\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 35.6884 - mse: 35.6884 - val_loss: 22.5077 - val_mse: 22.5077\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 33.6865 - mse: 33.6865 - val_loss: 21.2852 - val_mse: 21.2852\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 32.1435 - mse: 32.1435 - val_loss: 20.2458 - val_mse: 20.2458\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 30.8608 - mse: 30.8608 - val_loss: 19.1066 - val_mse: 19.1066\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 29.7524 - mse: 29.7524 - val_loss: 18.8190 - val_mse: 18.8190\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28.7969 - mse: 28.7969 - val_loss: 18.0505 - val_mse: 18.0505\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 27.9486 - mse: 27.9486 - val_loss: 17.6376 - val_mse: 17.6376\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 27.2275 - mse: 27.2275 - val_loss: 16.3004 - val_mse: 16.3004\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 26.3642 - mse: 26.3642 - val_loss: 16.4024 - val_mse: 16.4024\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 25.7193 - mse: 25.7193 - val_loss: 15.7691 - val_mse: 15.7691\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 25.0510 - mse: 25.0510 - val_loss: 15.4638 - val_mse: 15.4638\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 24.5012 - mse: 24.5012 - val_loss: 14.4397 - val_mse: 14.4397\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 23.9947 - mse: 23.9947 - val_loss: 14.7778 - val_mse: 14.7778\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 23.3437 - mse: 23.3437 - val_loss: 13.9304 - val_mse: 13.9304\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 22.8094 - mse: 22.8094 - val_loss: 13.5184 - val_mse: 13.5184\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 22.3504 - mse: 22.3504 - val_loss: 14.3002 - val_mse: 14.3002\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 21.7954 - mse: 21.7954 - val_loss: 13.1758 - val_mse: 13.1758\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 21.3164 - mse: 21.3164 - val_loss: 13.2853 - val_mse: 13.2853\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 20.7427 - mse: 20.7427 - val_loss: 12.6131 - val_mse: 12.6131\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 20.3400 - mse: 20.3400 - val_loss: 12.3213 - val_mse: 12.3213\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 19.9452 - mse: 19.9452 - val_loss: 12.5110 - val_mse: 12.5110\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 19.5131 - mse: 19.5131 - val_loss: 12.1465 - val_mse: 12.1465\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 19.1104 - mse: 19.1104 - val_loss: 12.2338 - val_mse: 12.2338\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 18.8668 - mse: 18.8668 - val_loss: 12.0144 - val_mse: 12.0144\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 18.3875 - mse: 18.3875 - val_loss: 11.4090 - val_mse: 11.4090\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 18.1249 - mse: 18.1249 - val_loss: 11.1449 - val_mse: 11.1449\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 17.8280 - mse: 17.8280 - val_loss: 10.8771 - val_mse: 10.8771\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 17.4869 - mse: 17.4869 - val_loss: 11.9984 - val_mse: 11.9984\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 17.1434 - mse: 17.1434 - val_loss: 10.7404 - val_mse: 10.7404\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 16.7145 - mse: 16.7145 - val_loss: 10.7956 - val_mse: 10.7956\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 16.3637 - mse: 16.3637 - val_loss: 11.2443 - val_mse: 11.2443\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 15.9068 - mse: 15.9068 - val_loss: 10.7985 - val_mse: 10.7985\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 15.5737 - mse: 15.5737 - val_loss: 10.9151 - val_mse: 10.9151\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 15.2325 - mse: 15.2325 - val_loss: 10.8671 - val_mse: 10.8671\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 14.9694 - mse: 14.9694 - val_loss: 10.8904 - val_mse: 10.8904\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 14.5689 - mse: 14.5689 - val_loss: 10.7942 - val_mse: 10.7942\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 14.3387 - mse: 14.3387 - val_loss: 10.8025 - val_mse: 10.8025\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 14.0554 - mse: 14.0554 - val_loss: 10.5804 - val_mse: 10.5804\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.7143 - mse: 13.7143 - val_loss: 11.0347 - val_mse: 11.0347\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 13.6256 - mse: 13.6256 - val_loss: 10.9456 - val_mse: 10.9456\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.2918 - mse: 13.2918 - val_loss: 10.7446 - val_mse: 10.7446\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 13.1637 - mse: 13.1637 - val_loss: 11.3525 - val_mse: 11.3525\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.7848 - mse: 12.7848 - val_loss: 10.7784 - val_mse: 10.7784\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 12.5595 - mse: 12.5595 - val_loss: 10.8347 - val_mse: 10.8347\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.3580 - mse: 12.3580 - val_loss: 10.7663 - val_mse: 10.7663\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.1685 - mse: 12.1685 - val_loss: 10.8187 - val_mse: 10.8187\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 12.0225 - mse: 12.0225 - val_loss: 10.7185 - val_mse: 10.7185\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.7949 - mse: 11.7949 - val_loss: 10.9310 - val_mse: 10.9310\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.7057 - mse: 11.7057 - val_loss: 10.9534 - val_mse: 10.9534\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.5078 - mse: 11.5078 - val_loss: 10.8350 - val_mse: 10.8350\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.5448 - mse: 11.5448 - val_loss: 11.0365 - val_mse: 11.0365\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.2693 - mse: 11.2693 - val_loss: 10.9442 - val_mse: 10.9442\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 11.1175 - mse: 11.1175 - val_loss: 10.9916 - val_mse: 10.9916\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.9532 - mse: 10.9532 - val_loss: 10.9671 - val_mse: 10.9671\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.9579 - mse: 10.9579 - val_loss: 10.7277 - val_mse: 10.7277\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.7539 - mse: 10.7539 - val_loss: 11.1288 - val_mse: 11.1288\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.6868 - mse: 10.6868 - val_loss: 11.0902 - val_mse: 11.0902\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.5199 - mse: 10.5199 - val_loss: 11.0507 - val_mse: 11.0507\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.4073 - mse: 10.4073 - val_loss: 11.2212 - val_mse: 11.2212\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.3969 - mse: 10.3969 - val_loss: 10.9790 - val_mse: 10.9790\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.3663 - mse: 10.3663 - val_loss: 10.9072 - val_mse: 10.9072\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.1102 - mse: 10.1102 - val_loss: 11.2774 - val_mse: 11.2774\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.0475 - mse: 10.0475 - val_loss: 11.0228 - val_mse: 11.0228\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.9647 - mse: 9.9647 - val_loss: 11.1225 - val_mse: 11.1225\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.7480 - mse: 9.7480 - val_loss: 11.1148 - val_mse: 11.1148\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.7357 - mse: 9.7357 - val_loss: 11.4395 - val_mse: 11.4395\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.6778 - mse: 9.6778 - val_loss: 11.1161 - val_mse: 11.1161\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.5748 - mse: 9.5748 - val_loss: 11.4315 - val_mse: 11.4315\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.5283 - mse: 9.5283 - val_loss: 11.1697 - val_mse: 11.1697\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.3757 - mse: 9.3757 - val_loss: 11.3821 - val_mse: 11.3821\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.3434 - mse: 9.3434 - val_loss: 11.0506 - val_mse: 11.0506\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.2480 - mse: 9.2480 - val_loss: 11.3121 - val_mse: 11.3121\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.1479 - mse: 9.1479 - val_loss: 11.2700 - val_mse: 11.2700\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.1337 - mse: 9.1337 - val_loss: 11.5828 - val_mse: 11.5828\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.0069 - mse: 9.0069 - val_loss: 11.3270 - val_mse: 11.3270\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.9764 - mse: 8.9764 - val_loss: 11.2684 - val_mse: 11.2684\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.9139 - mse: 8.9139 - val_loss: 11.3205 - val_mse: 11.3205\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.8464 - mse: 8.8464 - val_loss: 11.3315 - val_mse: 11.3315\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.8721 - mse: 8.8721 - val_loss: 11.1582 - val_mse: 11.1582\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.7735 - mse: 8.7735 - val_loss: 11.1108 - val_mse: 11.1108\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.6598 - mse: 8.6598 - val_loss: 11.0759 - val_mse: 11.0759\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.6777 - mse: 8.6777 - val_loss: 11.0709 - val_mse: 11.0709\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.5127 - mse: 8.5127 - val_loss: 11.1878 - val_mse: 11.1878\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.5261 - mse: 8.5261 - val_loss: 11.2034 - val_mse: 11.2034\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.4719 - mse: 8.4719 - val_loss: 11.1067 - val_mse: 11.1067\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.4440 - mse: 8.4440 - val_loss: 10.9234 - val_mse: 10.9234\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.4784 - mse: 8.4784 - val_loss: 11.2162 - val_mse: 11.2162\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.3132 - mse: 8.3132 - val_loss: 11.0137 - val_mse: 11.0137\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.2380 - mse: 8.2380 - val_loss: 10.9658 - val_mse: 10.9658\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.2200 - mse: 8.2200 - val_loss: 10.9793 - val_mse: 10.9793\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.1918 - mse: 8.1918 - val_loss: 11.1407 - val_mse: 11.1407\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.1325 - mse: 8.1325 - val_loss: 11.4571 - val_mse: 11.4571\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.0966 - mse: 8.0966 - val_loss: 11.0664 - val_mse: 11.0664\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.0074 - mse: 8.0074 - val_loss: 11.3438 - val_mse: 11.3438\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.9457 - mse: 7.9457 - val_loss: 11.4012 - val_mse: 11.4012\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.9163 - mse: 7.9163 - val_loss: 11.2795 - val_mse: 11.2795\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.9095 - mse: 7.9095 - val_loss: 11.2235 - val_mse: 11.2235\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.8060 - mse: 7.8060 - val_loss: 11.1510 - val_mse: 11.1510\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8676 - mse: 7.8676 - val_loss: 11.1096 - val_mse: 11.1096\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.7906 - mse: 7.7906 - val_loss: 10.8915 - val_mse: 10.8915\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.7730 - mse: 7.7730 - val_loss: 11.0303 - val_mse: 11.0303\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.7007 - mse: 7.7007 - val_loss: 11.1045 - val_mse: 11.1045\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.7190 - mse: 7.7190 - val_loss: 11.2721 - val_mse: 11.2721\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.6643 - mse: 7.6643 - val_loss: 11.1588 - val_mse: 11.1588\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.7972 - mse: 7.7972 - val_loss: 11.3340 - val_mse: 11.3340\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.6718 - mse: 7.6718 - val_loss: 11.0068 - val_mse: 11.0068\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.6563 - mse: 7.6563 - val_loss: 11.1618 - val_mse: 11.1618\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.5554 - mse: 7.5554 - val_loss: 11.0520 - val_mse: 11.0520\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5506 - mse: 7.5506 - val_loss: 11.1148 - val_mse: 11.1148\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.5258 - mse: 7.5258 - val_loss: 11.1774 - val_mse: 11.1774\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.5291 - mse: 7.5291 - val_loss: 11.0253 - val_mse: 11.0253\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.4955 - mse: 7.4955 - val_loss: 11.4534 - val_mse: 11.4534\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.3916 - mse: 7.3916 - val_loss: 11.1298 - val_mse: 11.1298\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.4637 - mse: 7.4637 - val_loss: 10.9989 - val_mse: 10.9989\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.4729 - mse: 7.4729 - val_loss: 11.3312 - val_mse: 11.3312\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.3322 - mse: 7.3322 - val_loss: 11.1059 - val_mse: 11.1059\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2820 - mse: 7.2820 - val_loss: 11.0980 - val_mse: 11.0980\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.3207 - mse: 7.3207 - val_loss: 11.0818 - val_mse: 11.0818\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.3186 - mse: 7.3186 - val_loss: 11.4051 - val_mse: 11.4051\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2340 - mse: 7.2340 - val_loss: 11.1245 - val_mse: 11.1245\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2200 - mse: 7.2200 - val_loss: 11.1104 - val_mse: 11.1104\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1956 - mse: 7.1956 - val_loss: 11.2127 - val_mse: 11.2127\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2882 - mse: 7.2882 - val_loss: 11.4163 - val_mse: 11.4163\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2269 - mse: 7.2269 - val_loss: 11.2543 - val_mse: 11.2543\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2076 - mse: 7.2076 - val_loss: 11.1651 - val_mse: 11.1651\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0824 - mse: 7.0824 - val_loss: 10.8590 - val_mse: 10.8590\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.0154 - mse: 7.0154 - val_loss: 11.1972 - val_mse: 11.1972\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0235 - mse: 7.0235 - val_loss: 11.1113 - val_mse: 11.1113\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.0604 - mse: 7.0604 - val_loss: 11.1605 - val_mse: 11.1605\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0059 - mse: 7.0059 - val_loss: 11.1230 - val_mse: 11.1230\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9783 - mse: 6.9783 - val_loss: 11.0719 - val_mse: 11.0719\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0175 - mse: 7.0175 - val_loss: 11.2221 - val_mse: 11.2221\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.9786 - mse: 6.9786 - val_loss: 11.6663 - val_mse: 11.6663\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.9225 - mse: 6.9225 - val_loss: 11.4928 - val_mse: 11.4928\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.8974 - mse: 6.8974 - val_loss: 11.1023 - val_mse: 11.1023\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8516 - mse: 6.8516 - val_loss: 11.2932 - val_mse: 11.2932\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7630 - mse: 6.7630 - val_loss: 11.4250 - val_mse: 11.4250\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.8335 - mse: 6.8335 - val_loss: 11.3172 - val_mse: 11.3172\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7740 - mse: 6.7740 - val_loss: 11.5435 - val_mse: 11.5435\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7006 - mse: 6.7006 - val_loss: 11.5757 - val_mse: 11.5757\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.6876 - mse: 6.6876 - val_loss: 11.3868 - val_mse: 11.3868\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.6664 - mse: 6.6664 - val_loss: 11.6776 - val_mse: 11.6776\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6449 - mse: 6.6449 - val_loss: 11.5546 - val_mse: 11.5546\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.6923 - mse: 6.6923 - val_loss: 11.5462 - val_mse: 11.5462\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6076 - mse: 6.6076 - val_loss: 11.4408 - val_mse: 11.4408\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6410 - mse: 6.6410 - val_loss: 11.2515 - val_mse: 11.2515\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5779 - mse: 6.5779 - val_loss: 11.6253 - val_mse: 11.6253\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5305 - mse: 6.5305 - val_loss: 11.4095 - val_mse: 11.4095\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5899 - mse: 6.5899 - val_loss: 11.2458 - val_mse: 11.2458\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5551 - mse: 6.5551 - val_loss: 11.7117 - val_mse: 11.7117\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.4665 - mse: 6.4665 - val_loss: 11.4648 - val_mse: 11.4648\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3949 - mse: 6.3949 - val_loss: 11.2795 - val_mse: 11.2795\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.4260 - mse: 6.4260 - val_loss: 11.5082 - val_mse: 11.5082\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.4604 - mse: 6.4604 - val_loss: 11.4653 - val_mse: 11.4653\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3604 - mse: 6.3604 - val_loss: 11.2239 - val_mse: 11.2239\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3933 - mse: 6.3933 - val_loss: 11.3601 - val_mse: 11.3601\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3301 - mse: 6.3301 - val_loss: 11.6027 - val_mse: 11.6027\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3232 - mse: 6.3232 - val_loss: 11.2592 - val_mse: 11.2592\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3522 - mse: 6.3522 - val_loss: 11.2887 - val_mse: 11.2887\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3067 - mse: 6.3067 - val_loss: 11.0913 - val_mse: 11.0913\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3037 - mse: 6.3037 - val_loss: 11.2857 - val_mse: 11.2857\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3084 - mse: 6.3084 - val_loss: 11.2696 - val_mse: 11.2696\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2233 - mse: 6.2233 - val_loss: 11.5618 - val_mse: 11.5618\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2511 - mse: 6.2511 - val_loss: 11.4653 - val_mse: 11.4653\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.1324 - mse: 6.1324 - val_loss: 11.2456 - val_mse: 11.2456\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.1853 - mse: 6.1853 - val_loss: 11.3594 - val_mse: 11.3594\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1713 - mse: 6.1713 - val_loss: 11.2540 - val_mse: 11.2540\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1156 - mse: 6.1156 - val_loss: 11.1733 - val_mse: 11.1733\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0365 - mse: 6.0365 - val_loss: 11.3132 - val_mse: 11.3132\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0305 - mse: 6.0305 - val_loss: 11.5179 - val_mse: 11.5179\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0855 - mse: 6.0855 - val_loss: 11.2004 - val_mse: 11.2004\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9522 - mse: 5.9522 - val_loss: 11.3331 - val_mse: 11.3331\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.9809 - mse: 5.9809 - val_loss: 11.3448 - val_mse: 11.3448\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.9987 - mse: 5.9987 - val_loss: 11.2487 - val_mse: 11.2487\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8965 - mse: 5.8965 - val_loss: 11.2275 - val_mse: 11.2275\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8854 - mse: 5.8854 - val_loss: 11.2699 - val_mse: 11.2699\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9648 - mse: 5.9648 - val_loss: 11.0650 - val_mse: 11.0650\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8152 - mse: 5.8152 - val_loss: 11.4185 - val_mse: 11.4185\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8772 - mse: 5.8772 - val_loss: 11.2874 - val_mse: 11.2874\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8309 - mse: 5.8309 - val_loss: 11.0077 - val_mse: 11.0077\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.9585 - mse: 5.9585 - val_loss: 11.4037 - val_mse: 11.4037\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.9466 - mse: 5.9466 - val_loss: 11.4008 - val_mse: 11.4008\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.7949 - mse: 5.7949 - val_loss: 11.1189 - val_mse: 11.1189\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.7641 - mse: 5.7641 - val_loss: 11.4572 - val_mse: 11.4572\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.6756 - mse: 5.6756 - val_loss: 11.1541 - val_mse: 11.1541\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.6548 - mse: 5.6548 - val_loss: 10.9892 - val_mse: 10.9892\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.6905 - mse: 5.6905 - val_loss: 11.0671 - val_mse: 11.0671\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.7691 - mse: 5.7691 - val_loss: 11.2603 - val_mse: 11.2603\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.6191 - mse: 5.6191 - val_loss: 11.0337 - val_mse: 11.0337\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.6240 - mse: 5.6240 - val_loss: 11.0955 - val_mse: 11.0955\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5210 - mse: 5.5210 - val_loss: 11.2494 - val_mse: 11.2494\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5864 - mse: 5.5864 - val_loss: 11.0873 - val_mse: 11.0873\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.5386 - mse: 5.5386 - val_loss: 10.9261 - val_mse: 10.9261\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.4838 - mse: 5.4838 - val_loss: 11.2718 - val_mse: 11.2718\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.4432 - mse: 5.4432 - val_loss: 10.8862 - val_mse: 10.8862\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.4207 - mse: 5.4207 - val_loss: 10.8344 - val_mse: 10.8344\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.4170 - mse: 5.4170 - val_loss: 11.0165 - val_mse: 11.0165\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.4600 - mse: 5.4600 - val_loss: 11.0435 - val_mse: 11.0435\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.3778 - mse: 5.3778 - val_loss: 10.7734 - val_mse: 10.7734\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.3757 - mse: 5.3757 - val_loss: 11.1964 - val_mse: 11.1964\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.3644 - mse: 5.3644 - val_loss: 11.0642 - val_mse: 11.0642\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.3167 - mse: 5.3167 - val_loss: 10.6562 - val_mse: 10.6562\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.3541 - mse: 5.3541 - val_loss: 11.0193 - val_mse: 11.0193\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.3181 - mse: 5.3181 - val_loss: 11.0568 - val_mse: 11.0568\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.2409 - mse: 5.2409 - val_loss: 10.6291 - val_mse: 10.6291\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.2294 - mse: 5.2294 - val_loss: 10.6113 - val_mse: 10.6113\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.2578 - mse: 5.2578 - val_loss: 10.9118 - val_mse: 10.9118\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.1821 - mse: 5.1821 - val_loss: 10.7732 - val_mse: 10.7732\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.1846 - mse: 5.1846 - val_loss: 11.0327 - val_mse: 11.0327\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.1622 - mse: 5.1622 - val_loss: 10.8654 - val_mse: 10.8654\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.1512 - mse: 5.1512 - val_loss: 10.8999 - val_mse: 10.8999\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.1520 - mse: 5.1520 - val_loss: 10.8945 - val_mse: 10.8945\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.3089 - mse: 5.3089 - val_loss: 10.6110 - val_mse: 10.6110\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.1392 - mse: 5.1392 - val_loss: 10.7537 - val_mse: 10.7537\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.1866 - mse: 5.1866 - val_loss: 10.4364 - val_mse: 10.4364\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.0958 - mse: 5.0958 - val_loss: 10.6887 - val_mse: 10.6887\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.0870 - mse: 5.0870 - val_loss: 10.6910 - val_mse: 10.6910\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.1405 - mse: 5.1405 - val_loss: 10.6304 - val_mse: 10.6304\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.0826 - mse: 5.0826 - val_loss: 10.7616 - val_mse: 10.7616\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.0531 - mse: 5.0531 - val_loss: 10.7435 - val_mse: 10.7435\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.9914 - mse: 4.9914 - val_loss: 10.6008 - val_mse: 10.6008\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.0573 - mse: 5.0573 - val_loss: 10.8842 - val_mse: 10.8842\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.0104 - mse: 5.0104 - val_loss: 10.9800 - val_mse: 10.9800\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.9707 - mse: 4.9707 - val_loss: 10.5486 - val_mse: 10.5486\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.1454 - mse: 5.1454 - val_loss: 10.8664 - val_mse: 10.8664\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.1048 - mse: 5.1048 - val_loss: 11.1665 - val_mse: 11.1665\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.9206 - mse: 4.9206 - val_loss: 10.4410 - val_mse: 10.4410\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.9343 - mse: 4.9343 - val_loss: 10.8278 - val_mse: 10.8278\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.9356 - mse: 4.9356 - val_loss: 10.8024 - val_mse: 10.8024\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.9229 - mse: 4.9229 - val_loss: 10.5785 - val_mse: 10.5785\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.9020 - mse: 4.9020 - val_loss: 10.6316 - val_mse: 10.6316\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.8625 - mse: 4.8625 - val_loss: 10.7518 - val_mse: 10.7518\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.8303 - mse: 4.8303 - val_loss: 10.7030 - val_mse: 10.7030\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.8579 - mse: 4.8579 - val_loss: 10.5340 - val_mse: 10.5340\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.7705 - mse: 4.7705 - val_loss: 10.7917 - val_mse: 10.7917\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.8197 - mse: 4.8197 - val_loss: 10.7397 - val_mse: 10.7397\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.8404 - mse: 4.8404 - val_loss: 10.4486 - val_mse: 10.4486\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb5228a72d0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEFaa0VQaKef"
      },
      "source": [
        "## Question 1\n",
        "\n",
        "Now that you know how MSE works, you need to plot the behavior of MSE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc5OFsCmadXE"
      },
      "source": [
        "### Answer 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8RF_LUDbdo2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "cabeeda2-3410-4246-aa08-ae058b75f616"
      },
      "source": [
        "errors = np.random.randint(5, size=(2, 4))\n",
        "n = len(errors)\n",
        "\n",
        "mse = np.square(errors)/n\n",
        "\n",
        "plt.plot(errors, mse, c='#ED4F46', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Difference between actual and estimated ouputs')\n",
        "plt.ylabel('Mean Squared Errors')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aShHwxvw6hml"
      },
      "source": [
        "## Mean Absolute Error [MAE]\n",
        "\n",
        "Mean absolute error, on the other hand, is measured as the average of sum of absolute differences between predictions and actual observations. \n",
        "\n",
        "Like MSE, this as well measures the magnitude of error without considering their direction. \n",
        "\n",
        "Unlike MSE, MAE needs more complicated tools such as linear programming to compute the gradients. Plus MAE is more robust to outliers since it does not make use of square.\n",
        "\n",
        "Let's assume there are $n$ data samples, for $i^{th}$ sample; the actual output is $y_i$ and $\\hat{y}_i$ is the estimated output from the regression model. \n",
        "\n",
        "We first take the absolute difference between the original and estimated output with $|y_i - \\hat{y}_i|2$. Then we take sum of the absolute differences for all the samples. And finally divide it by the total count of samples, which is $n$. \n",
        "\n",
        "$$MSE = \\frac{\\sum_{i=1}^{n}|y_i - \\hat{y}_i|}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI9_GEL86hmn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d9e4efa-6cdd-44f6-9739-9577ce72d393"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='mae',\n",
        "              metrics=['mae'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 1s 15ms/step - loss: 22.4378 - mae: 22.4378 - val_loss: 21.0604 - val_mae: 21.0604\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 21.6559 - mae: 21.6559 - val_loss: 20.0907 - val_mae: 20.0907\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 20.4429 - mae: 20.4429 - val_loss: 18.4795 - val_mae: 18.4795\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 18.3756 - mae: 18.3756 - val_loss: 15.7994 - val_mae: 15.7994\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 15.1623 - mae: 15.1623 - val_loss: 11.8758 - val_mae: 11.8758\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.7556 - mae: 10.7556 - val_loss: 7.4920 - val_mae: 7.4920\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.4751 - mae: 7.4751 - val_loss: 6.4273 - val_mae: 6.4273\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2168 - mae: 6.2168 - val_loss: 5.2925 - val_mae: 5.2925\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.8039 - mae: 4.8039 - val_loss: 4.1064 - val_mae: 4.1064\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3.8489 - mae: 3.8489 - val_loss: 3.6813 - val_mae: 3.6813\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3.4713 - mae: 3.4713 - val_loss: 3.2796 - val_mae: 3.2796\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3.3185 - mae: 3.3185 - val_loss: 3.3205 - val_mae: 3.3205\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3.1210 - mae: 3.1210 - val_loss: 3.0984 - val_mae: 3.0984\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.9870 - mae: 2.9870 - val_loss: 3.0379 - val_mae: 3.0379\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.8858 - mae: 2.8858 - val_loss: 2.8116 - val_mae: 2.8116\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.7529 - mae: 2.7529 - val_loss: 2.8439 - val_mae: 2.8439\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.6852 - mae: 2.6852 - val_loss: 2.5774 - val_mae: 2.5774\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.6049 - mae: 2.6049 - val_loss: 2.5782 - val_mae: 2.5782\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.5379 - mae: 2.5379 - val_loss: 2.5267 - val_mae: 2.5267\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.4909 - mae: 2.4909 - val_loss: 2.5073 - val_mae: 2.5073\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.4559 - mae: 2.4559 - val_loss: 2.4823 - val_mae: 2.4823\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.3661 - mae: 2.3661 - val_loss: 2.3779 - val_mae: 2.3779\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.3221 - mae: 2.3221 - val_loss: 2.4781 - val_mae: 2.4781\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.3268 - mae: 2.3268 - val_loss: 2.3770 - val_mae: 2.3770\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.2512 - mae: 2.2512 - val_loss: 2.3980 - val_mae: 2.3980\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.2638 - mae: 2.2638 - val_loss: 2.3353 - val_mae: 2.3353\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.2408 - mae: 2.2408 - val_loss: 2.2265 - val_mae: 2.2265\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.1875 - mae: 2.1875 - val_loss: 2.2522 - val_mae: 2.2522\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.1456 - mae: 2.1456 - val_loss: 2.2510 - val_mae: 2.2510\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.1344 - mae: 2.1344 - val_loss: 2.2197 - val_mae: 2.2197\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.1391 - mae: 2.1391 - val_loss: 2.1695 - val_mae: 2.1695\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.1093 - mae: 2.1093 - val_loss: 2.1905 - val_mae: 2.1905\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.0992 - mae: 2.0992 - val_loss: 2.3259 - val_mae: 2.3259\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.1184 - mae: 2.1184 - val_loss: 2.1335 - val_mae: 2.1335\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.0452 - mae: 2.0452 - val_loss: 2.2297 - val_mae: 2.2297\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.0884 - mae: 2.0884 - val_loss: 2.1688 - val_mae: 2.1688\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.0185 - mae: 2.0185 - val_loss: 2.1379 - val_mae: 2.1379\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.0528 - mae: 2.0528 - val_loss: 2.1810 - val_mae: 2.1810\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.9798 - mae: 1.9798 - val_loss: 2.1501 - val_mae: 2.1501\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.0155 - mae: 2.0155 - val_loss: 2.1555 - val_mae: 2.1555\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.9824 - mae: 1.9824 - val_loss: 2.1952 - val_mae: 2.1952\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.9447 - mae: 1.9447 - val_loss: 2.1068 - val_mae: 2.1068\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.9731 - mae: 1.9731 - val_loss: 2.0664 - val_mae: 2.0664\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.9674 - mae: 1.9674 - val_loss: 2.1486 - val_mae: 2.1486\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.9341 - mae: 1.9341 - val_loss: 2.0206 - val_mae: 2.0206\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.9234 - mae: 1.9234 - val_loss: 2.0998 - val_mae: 2.0998\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.9136 - mae: 1.9136 - val_loss: 2.2430 - val_mae: 2.2430\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.9311 - mae: 1.9311 - val_loss: 2.1215 - val_mae: 2.1215\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.8860 - mae: 1.8860 - val_loss: 2.0877 - val_mae: 2.0877\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8568 - mae: 1.8568 - val_loss: 2.0979 - val_mae: 2.0979\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.8542 - mae: 1.8542 - val_loss: 2.0929 - val_mae: 2.0929\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8303 - mae: 1.8303 - val_loss: 2.0002 - val_mae: 2.0002\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8320 - mae: 1.8320 - val_loss: 2.1542 - val_mae: 2.1542\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8388 - mae: 1.8388 - val_loss: 2.0342 - val_mae: 2.0342\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.8179 - mae: 1.8179 - val_loss: 2.1542 - val_mae: 2.1542\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8083 - mae: 1.8083 - val_loss: 2.1097 - val_mae: 2.1097\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.7852 - mae: 1.7852 - val_loss: 2.1151 - val_mae: 2.1151\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7858 - mae: 1.7858 - val_loss: 2.0984 - val_mae: 2.0984\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7678 - mae: 1.7678 - val_loss: 2.0720 - val_mae: 2.0720\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.7557 - mae: 1.7557 - val_loss: 2.1355 - val_mae: 2.1355\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7567 - mae: 1.7567 - val_loss: 2.1408 - val_mae: 2.1408\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7543 - mae: 1.7543 - val_loss: 2.0291 - val_mae: 2.0291\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7194 - mae: 1.7194 - val_loss: 2.1149 - val_mae: 2.1149\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7201 - mae: 1.7201 - val_loss: 2.0781 - val_mae: 2.0781\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7046 - mae: 1.7046 - val_loss: 2.0687 - val_mae: 2.0687\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.7770 - mae: 1.7770 - val_loss: 2.1780 - val_mae: 2.1780\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7073 - mae: 1.7073 - val_loss: 2.1088 - val_mae: 2.1088\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.7187 - mae: 1.7187 - val_loss: 2.1020 - val_mae: 2.1020\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.7334 - mae: 1.7334 - val_loss: 2.1165 - val_mae: 2.1165\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6831 - mae: 1.6831 - val_loss: 2.0815 - val_mae: 2.0815\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7168 - mae: 1.7168 - val_loss: 2.1283 - val_mae: 2.1283\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7008 - mae: 1.7008 - val_loss: 2.1549 - val_mae: 2.1549\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7645 - mae: 1.7645 - val_loss: 2.2324 - val_mae: 2.2324\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6851 - mae: 1.6851 - val_loss: 2.0601 - val_mae: 2.0601\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6894 - mae: 1.6894 - val_loss: 2.1789 - val_mae: 2.1789\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.7218 - mae: 1.7218 - val_loss: 2.1880 - val_mae: 2.1880\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6915 - mae: 1.6915 - val_loss: 2.1205 - val_mae: 2.1205\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6430 - mae: 1.6430 - val_loss: 2.1441 - val_mae: 2.1441\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6672 - mae: 1.6672 - val_loss: 2.1525 - val_mae: 2.1525\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6401 - mae: 1.6401 - val_loss: 2.0596 - val_mae: 2.0596\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6338 - mae: 1.6338 - val_loss: 2.1828 - val_mae: 2.1828\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5669 - mae: 1.5669 - val_loss: 2.1034 - val_mae: 2.1034\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6053 - mae: 1.6053 - val_loss: 2.1207 - val_mae: 2.1207\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5437 - mae: 1.5437 - val_loss: 2.1056 - val_mae: 2.1056\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5189 - mae: 1.5189 - val_loss: 2.1252 - val_mae: 2.1252\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5358 - mae: 1.5358 - val_loss: 2.1367 - val_mae: 2.1367\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5325 - mae: 1.5325 - val_loss: 2.1524 - val_mae: 2.1524\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5199 - mae: 1.5199 - val_loss: 2.1256 - val_mae: 2.1256\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5169 - mae: 1.5169 - val_loss: 2.1623 - val_mae: 2.1623\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5335 - mae: 1.5335 - val_loss: 2.1077 - val_mae: 2.1077\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5458 - mae: 1.5458 - val_loss: 2.0931 - val_mae: 2.0931\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4998 - mae: 1.4998 - val_loss: 2.1668 - val_mae: 2.1668\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5212 - mae: 1.5212 - val_loss: 2.1036 - val_mae: 2.1036\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5148 - mae: 1.5148 - val_loss: 2.1164 - val_mae: 2.1164\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4992 - mae: 1.4992 - val_loss: 2.1909 - val_mae: 2.1909\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5200 - mae: 1.5200 - val_loss: 2.1492 - val_mae: 2.1492\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5357 - mae: 1.5357 - val_loss: 2.1519 - val_mae: 2.1519\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5020 - mae: 1.5020 - val_loss: 2.1325 - val_mae: 2.1325\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4295 - mae: 1.4295 - val_loss: 2.1223 - val_mae: 2.1223\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4204 - mae: 1.4204 - val_loss: 2.1150 - val_mae: 2.1150\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4316 - mae: 1.4316 - val_loss: 2.1244 - val_mae: 2.1244\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4439 - mae: 1.4439 - val_loss: 2.0995 - val_mae: 2.0995\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4361 - mae: 1.4361 - val_loss: 2.1228 - val_mae: 2.1228\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4185 - mae: 1.4185 - val_loss: 2.0901 - val_mae: 2.0901\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4520 - mae: 1.4520 - val_loss: 2.1426 - val_mae: 2.1426\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3915 - mae: 1.3915 - val_loss: 2.1884 - val_mae: 2.1884\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3848 - mae: 1.3848 - val_loss: 2.1551 - val_mae: 2.1551\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3691 - mae: 1.3691 - val_loss: 2.1214 - val_mae: 2.1214\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3995 - mae: 1.3995 - val_loss: 2.1548 - val_mae: 2.1548\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3645 - mae: 1.3645 - val_loss: 2.1146 - val_mae: 2.1146\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4049 - mae: 1.4049 - val_loss: 2.1372 - val_mae: 2.1372\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4607 - mae: 1.4607 - val_loss: 2.1537 - val_mae: 2.1537\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3866 - mae: 1.3866 - val_loss: 2.1652 - val_mae: 2.1652\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4468 - mae: 1.4468 - val_loss: 2.1259 - val_mae: 2.1259\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4581 - mae: 1.4581 - val_loss: 2.1426 - val_mae: 2.1426\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3727 - mae: 1.3727 - val_loss: 2.1819 - val_mae: 2.1819\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3978 - mae: 1.3978 - val_loss: 2.1448 - val_mae: 2.1448\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3565 - mae: 1.3565 - val_loss: 2.1473 - val_mae: 2.1473\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3374 - mae: 1.3374 - val_loss: 2.1219 - val_mae: 2.1219\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3899 - mae: 1.3899 - val_loss: 2.1499 - val_mae: 2.1499\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3647 - mae: 1.3647 - val_loss: 2.1455 - val_mae: 2.1455\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3598 - mae: 1.3598 - val_loss: 2.1514 - val_mae: 2.1514\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3411 - mae: 1.3411 - val_loss: 2.0939 - val_mae: 2.0939\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3158 - mae: 1.3158 - val_loss: 2.1336 - val_mae: 2.1336\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3298 - mae: 1.3298 - val_loss: 2.1995 - val_mae: 2.1995\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3214 - mae: 1.3214 - val_loss: 2.1482 - val_mae: 2.1482\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3066 - mae: 1.3066 - val_loss: 2.1042 - val_mae: 2.1042\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3155 - mae: 1.3155 - val_loss: 2.1146 - val_mae: 2.1146\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3181 - mae: 1.3181 - val_loss: 2.0865 - val_mae: 2.0865\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2808 - mae: 1.2808 - val_loss: 2.1140 - val_mae: 2.1140\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3307 - mae: 1.3307 - val_loss: 2.1038 - val_mae: 2.1038\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3397 - mae: 1.3397 - val_loss: 2.1083 - val_mae: 2.1083\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3001 - mae: 1.3001 - val_loss: 2.0761 - val_mae: 2.0761\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2815 - mae: 1.2815 - val_loss: 2.1743 - val_mae: 2.1743\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2620 - mae: 1.2620 - val_loss: 2.1032 - val_mae: 2.1032\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2650 - mae: 1.2650 - val_loss: 2.1188 - val_mae: 2.1188\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3333 - mae: 1.3333 - val_loss: 2.1402 - val_mae: 2.1402\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3513 - mae: 1.3513 - val_loss: 2.1249 - val_mae: 2.1249\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3171 - mae: 1.3171 - val_loss: 2.1425 - val_mae: 2.1425\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3658 - mae: 1.3658 - val_loss: 2.1506 - val_mae: 2.1506\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2576 - mae: 1.2576 - val_loss: 2.1087 - val_mae: 2.1087\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2963 - mae: 1.2963 - val_loss: 2.0985 - val_mae: 2.0985\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2712 - mae: 1.2712 - val_loss: 2.0521 - val_mae: 2.0521\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2647 - mae: 1.2647 - val_loss: 2.0811 - val_mae: 2.0811\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2391 - mae: 1.2391 - val_loss: 2.1200 - val_mae: 2.1200\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2510 - mae: 1.2510 - val_loss: 2.0905 - val_mae: 2.0905\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2312 - mae: 1.2312 - val_loss: 2.0481 - val_mae: 2.0481\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2366 - mae: 1.2366 - val_loss: 2.0690 - val_mae: 2.0690\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2325 - mae: 1.2325 - val_loss: 2.1023 - val_mae: 2.1023\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2198 - mae: 1.2198 - val_loss: 2.0525 - val_mae: 2.0525\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2252 - mae: 1.2252 - val_loss: 2.0601 - val_mae: 2.0601\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2499 - mae: 1.2499 - val_loss: 2.0606 - val_mae: 2.0606\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2840 - mae: 1.2840 - val_loss: 2.1091 - val_mae: 2.1091\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2629 - mae: 1.2629 - val_loss: 2.0939 - val_mae: 2.0939\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2027 - mae: 1.2027 - val_loss: 2.0681 - val_mae: 2.0681\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2038 - mae: 1.2038 - val_loss: 2.0536 - val_mae: 2.0536\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2169 - mae: 1.2169 - val_loss: 2.0937 - val_mae: 2.0937\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2654 - mae: 1.2654 - val_loss: 2.0641 - val_mae: 2.0641\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2127 - mae: 1.2127 - val_loss: 2.0567 - val_mae: 2.0567\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2478 - mae: 1.2478 - val_loss: 2.0518 - val_mae: 2.0518\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1899 - mae: 1.1899 - val_loss: 2.0316 - val_mae: 2.0316\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1737 - mae: 1.1737 - val_loss: 2.0280 - val_mae: 2.0280\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1609 - mae: 1.1609 - val_loss: 2.0317 - val_mae: 2.0317\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1855 - mae: 1.1855 - val_loss: 2.0886 - val_mae: 2.0886\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1742 - mae: 1.1742 - val_loss: 2.0286 - val_mae: 2.0286\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1946 - mae: 1.1946 - val_loss: 2.0136 - val_mae: 2.0136\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1754 - mae: 1.1754 - val_loss: 2.0579 - val_mae: 2.0579\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1667 - mae: 1.1667 - val_loss: 2.0819 - val_mae: 2.0819\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1755 - mae: 1.1755 - val_loss: 2.0445 - val_mae: 2.0445\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1806 - mae: 1.1806 - val_loss: 2.0518 - val_mae: 2.0518\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1974 - mae: 1.1974 - val_loss: 2.0107 - val_mae: 2.0107\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2113 - mae: 1.2113 - val_loss: 2.0360 - val_mae: 2.0360\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1811 - mae: 1.1811 - val_loss: 2.0164 - val_mae: 2.0164\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2500 - mae: 1.2500 - val_loss: 2.0306 - val_mae: 2.0306\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2049 - mae: 1.2049 - val_loss: 2.0002 - val_mae: 2.0002\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1450 - mae: 1.1450 - val_loss: 2.0288 - val_mae: 2.0288\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1378 - mae: 1.1378 - val_loss: 2.0185 - val_mae: 2.0185\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1288 - mae: 1.1288 - val_loss: 2.0021 - val_mae: 2.0021\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1316 - mae: 1.1316 - val_loss: 2.0136 - val_mae: 2.0136\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1866 - mae: 1.1866 - val_loss: 1.9928 - val_mae: 1.9928\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1884 - mae: 1.1884 - val_loss: 1.9874 - val_mae: 1.9874\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1241 - mae: 1.1241 - val_loss: 1.9963 - val_mae: 1.9963\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1246 - mae: 1.1246 - val_loss: 2.0094 - val_mae: 2.0094\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1433 - mae: 1.1433 - val_loss: 2.0043 - val_mae: 2.0043\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1379 - mae: 1.1379 - val_loss: 1.9653 - val_mae: 1.9653\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1368 - mae: 1.1368 - val_loss: 1.9545 - val_mae: 1.9545\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1831 - mae: 1.1831 - val_loss: 2.0329 - val_mae: 2.0329\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1400 - mae: 1.1400 - val_loss: 2.0244 - val_mae: 2.0244\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1173 - mae: 1.1173 - val_loss: 1.9674 - val_mae: 1.9674\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1095 - mae: 1.1095 - val_loss: 1.9591 - val_mae: 1.9591\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1038 - mae: 1.1038 - val_loss: 1.9649 - val_mae: 1.9649\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0984 - mae: 1.0984 - val_loss: 1.9370 - val_mae: 1.9370\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0951 - mae: 1.0951 - val_loss: 2.0024 - val_mae: 2.0024\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1283 - mae: 1.1283 - val_loss: 2.0211 - val_mae: 2.0211\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1468 - mae: 1.1468 - val_loss: 1.9587 - val_mae: 1.9587\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1311 - mae: 1.1311 - val_loss: 1.9902 - val_mae: 1.9902\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1114 - mae: 1.1114 - val_loss: 1.9807 - val_mae: 1.9807\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1534 - mae: 1.1534 - val_loss: 1.9746 - val_mae: 1.9746\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1392 - mae: 1.1392 - val_loss: 1.9837 - val_mae: 1.9837\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1766 - mae: 1.1766 - val_loss: 2.0328 - val_mae: 2.0328\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1086 - mae: 1.1086 - val_loss: 2.0434 - val_mae: 2.0434\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0821 - mae: 1.0821 - val_loss: 1.9551 - val_mae: 1.9551\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1543 - mae: 1.1543 - val_loss: 1.9630 - val_mae: 1.9630\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1461 - mae: 1.1461 - val_loss: 2.0608 - val_mae: 2.0608\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1429 - mae: 1.1429 - val_loss: 1.9697 - val_mae: 1.9697\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1070 - mae: 1.1070 - val_loss: 1.9419 - val_mae: 1.9419\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1305 - mae: 1.1305 - val_loss: 1.9461 - val_mae: 1.9461\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1094 - mae: 1.1094 - val_loss: 1.9642 - val_mae: 1.9642\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0787 - mae: 1.0787 - val_loss: 1.9282 - val_mae: 1.9282\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0787 - mae: 1.0787 - val_loss: 1.9743 - val_mae: 1.9743\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0913 - mae: 1.0913 - val_loss: 1.9391 - val_mae: 1.9391\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0671 - mae: 1.0671 - val_loss: 1.9558 - val_mae: 1.9558\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1070 - mae: 1.1070 - val_loss: 1.9360 - val_mae: 1.9360\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1441 - mae: 1.1441 - val_loss: 1.8952 - val_mae: 1.8952\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1833 - mae: 1.1833 - val_loss: 1.9736 - val_mae: 1.9736\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0906 - mae: 1.0906 - val_loss: 1.9904 - val_mae: 1.9904\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0981 - mae: 1.0981 - val_loss: 1.9332 - val_mae: 1.9332\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0847 - mae: 1.0847 - val_loss: 1.9686 - val_mae: 1.9686\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0653 - mae: 1.0653 - val_loss: 1.9076 - val_mae: 1.9076\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0942 - mae: 1.0942 - val_loss: 1.9285 - val_mae: 1.9285\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1413 - mae: 1.1413 - val_loss: 1.9810 - val_mae: 1.9810\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1071 - mae: 1.1071 - val_loss: 2.0107 - val_mae: 2.0107\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0564 - mae: 1.0564 - val_loss: 2.0011 - val_mae: 2.0011\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0564 - mae: 1.0564 - val_loss: 2.0092 - val_mae: 2.0092\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0685 - mae: 1.0685 - val_loss: 1.9302 - val_mae: 1.9302\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0382 - mae: 1.0382 - val_loss: 1.9403 - val_mae: 1.9403\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0337 - mae: 1.0337 - val_loss: 1.9490 - val_mae: 1.9490\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0175 - mae: 1.0175 - val_loss: 1.9625 - val_mae: 1.9625\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0398 - mae: 1.0398 - val_loss: 1.9751 - val_mae: 1.9751\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1016 - mae: 1.1016 - val_loss: 1.9480 - val_mae: 1.9480\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0448 - mae: 1.0448 - val_loss: 1.9956 - val_mae: 1.9956\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0680 - mae: 1.0680 - val_loss: 1.9828 - val_mae: 1.9828\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0702 - mae: 1.0702 - val_loss: 1.9466 - val_mae: 1.9466\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0505 - mae: 1.0505 - val_loss: 1.9357 - val_mae: 1.9357\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0522 - mae: 1.0522 - val_loss: 1.9466 - val_mae: 1.9466\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0248 - mae: 1.0248 - val_loss: 1.9479 - val_mae: 1.9479\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0412 - mae: 1.0412 - val_loss: 1.8682 - val_mae: 1.8682\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0255 - mae: 1.0255 - val_loss: 1.8893 - val_mae: 1.8893\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0044 - mae: 1.0044 - val_loss: 1.8893 - val_mae: 1.8893\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0091 - mae: 1.0091 - val_loss: 1.9280 - val_mae: 1.9280\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0373 - mae: 1.0373 - val_loss: 1.9440 - val_mae: 1.9440\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9929 - mae: 0.9929 - val_loss: 1.9103 - val_mae: 1.9103\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9908 - mae: 0.9908 - val_loss: 1.9162 - val_mae: 1.9162\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9892 - mae: 0.9892 - val_loss: 1.9313 - val_mae: 1.9313\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0105 - mae: 1.0105 - val_loss: 1.9401 - val_mae: 1.9401\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0412 - mae: 1.0412 - val_loss: 1.9716 - val_mae: 1.9716\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0028 - mae: 1.0028 - val_loss: 1.9233 - val_mae: 1.9233\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0072 - mae: 1.0072 - val_loss: 1.8948 - val_mae: 1.8948\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9898 - mae: 0.9898 - val_loss: 1.8866 - val_mae: 1.8866\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0039 - mae: 1.0039 - val_loss: 1.9087 - val_mae: 1.9087\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb4913a9810>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IEl3-k3bn7N"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "Now that you know how MAE works, you need to plot the behavior of MAE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYyGYH4zbn7N"
      },
      "source": [
        "### Answer 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK1qdGtBbn7N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "16efefde-b46e-4787-9f93-4440183e8623"
      },
      "source": [
        "errors = np.random.randint(5, size=(2, 4))\n",
        "n = len(errors)\n",
        "\n",
        "mae = np.abs(errors)/n\n",
        "\n",
        "plt.plot(errors, mae, c='#0095B6', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Difference between actual and estimated ouputs')\n",
        "plt.ylabel('Mean Absolute Errors')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIFJ88ER6s7w"
      },
      "source": [
        "## Mean Squared Logarithmic Error [MSLE]\n",
        "\n",
        "MSLE is just like MSE, but we have to take $log$ of the actual and estimated outputs because squaring and averaging. \n",
        "\n",
        "The introduction of the logarithm makes MSLE only care about the relative difference between the true and the predicted value, or in other words, it only cares about the percentual difference between them.\n",
        "\n",
        "This means that MSLE will treat small differences between small true and predicted values approximately the same as big differences between large true and predicted values.\n",
        "\n",
        "We can use MSLE when we don't want large errors to be significantly more penalized than small ones, in those cases where the range of the target value is large.\n",
        "\n",
        "*Example*: You want to predict future house prices, and your dataset includes homes that are orders of magnitude different in price. The price is a continuous value, and therefore, we want to do regression. MSLE can here be used as the loss function.\n",
        "\n",
        "$$MSLE = \\frac{\\sum_{i=1}^{n}(\\log(y_i+1) - \\log(\\hat{y}_i+1))^2}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBkzaP9R7KnB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1654fc0a-112e-4017-d074-7555d7db3763"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss=tf.keras.losses.MeanSquaredLogarithmicError(),\n",
        "              metrics=['mean_squared_logarithmic_error'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=150, validation_split = 0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "12/12 [==============================] - 1s 15ms/step - loss: 7.1424 - mean_squared_logarithmic_error: 7.1424 - val_loss: 5.2306 - val_mean_squared_logarithmic_error: 5.2306\n",
            "Epoch 2/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.2629 - mean_squared_logarithmic_error: 4.2629 - val_loss: 3.0720 - val_mean_squared_logarithmic_error: 3.0720\n",
            "Epoch 3/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.4779 - mean_squared_logarithmic_error: 2.4779 - val_loss: 1.6744 - val_mean_squared_logarithmic_error: 1.6744\n",
            "Epoch 4/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3578 - mean_squared_logarithmic_error: 1.3578 - val_loss: 0.8807 - val_mean_squared_logarithmic_error: 0.8807\n",
            "Epoch 5/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7366 - mean_squared_logarithmic_error: 0.7366 - val_loss: 0.4689 - val_mean_squared_logarithmic_error: 0.4689\n",
            "Epoch 6/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.4217 - mean_squared_logarithmic_error: 0.4217 - val_loss: 0.2670 - val_mean_squared_logarithmic_error: 0.2670\n",
            "Epoch 7/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2638 - mean_squared_logarithmic_error: 0.2638 - val_loss: 0.1719 - val_mean_squared_logarithmic_error: 0.1719\n",
            "Epoch 8/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1878 - mean_squared_logarithmic_error: 0.1878 - val_loss: 0.1251 - val_mean_squared_logarithmic_error: 0.1251\n",
            "Epoch 9/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1465 - mean_squared_logarithmic_error: 0.1465 - val_loss: 0.1003 - val_mean_squared_logarithmic_error: 0.1003\n",
            "Epoch 10/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1223 - mean_squared_logarithmic_error: 0.1223 - val_loss: 0.0848 - val_mean_squared_logarithmic_error: 0.0848\n",
            "Epoch 11/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1072 - mean_squared_logarithmic_error: 0.1072 - val_loss: 0.0732 - val_mean_squared_logarithmic_error: 0.0732\n",
            "Epoch 12/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0948 - mean_squared_logarithmic_error: 0.0948 - val_loss: 0.0651 - val_mean_squared_logarithmic_error: 0.0651\n",
            "Epoch 13/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0857 - mean_squared_logarithmic_error: 0.0857 - val_loss: 0.0587 - val_mean_squared_logarithmic_error: 0.0587\n",
            "Epoch 14/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0779 - mean_squared_logarithmic_error: 0.0779 - val_loss: 0.0532 - val_mean_squared_logarithmic_error: 0.0532\n",
            "Epoch 15/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0709 - mean_squared_logarithmic_error: 0.0709 - val_loss: 0.0485 - val_mean_squared_logarithmic_error: 0.0485\n",
            "Epoch 16/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0650 - mean_squared_logarithmic_error: 0.0650 - val_loss: 0.0439 - val_mean_squared_logarithmic_error: 0.0439\n",
            "Epoch 17/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0598 - mean_squared_logarithmic_error: 0.0598 - val_loss: 0.0402 - val_mean_squared_logarithmic_error: 0.0402\n",
            "Epoch 18/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0555 - mean_squared_logarithmic_error: 0.0555 - val_loss: 0.0368 - val_mean_squared_logarithmic_error: 0.0368\n",
            "Epoch 19/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0514 - mean_squared_logarithmic_error: 0.0514 - val_loss: 0.0340 - val_mean_squared_logarithmic_error: 0.0340\n",
            "Epoch 20/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0480 - mean_squared_logarithmic_error: 0.0480 - val_loss: 0.0317 - val_mean_squared_logarithmic_error: 0.0317\n",
            "Epoch 21/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0451 - mean_squared_logarithmic_error: 0.0451 - val_loss: 0.0298 - val_mean_squared_logarithmic_error: 0.0298\n",
            "Epoch 22/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0429 - mean_squared_logarithmic_error: 0.0429 - val_loss: 0.0282 - val_mean_squared_logarithmic_error: 0.0282\n",
            "Epoch 23/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0409 - mean_squared_logarithmic_error: 0.0409 - val_loss: 0.0269 - val_mean_squared_logarithmic_error: 0.0269\n",
            "Epoch 24/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0390 - mean_squared_logarithmic_error: 0.0390 - val_loss: 0.0264 - val_mean_squared_logarithmic_error: 0.0264\n",
            "Epoch 25/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0377 - mean_squared_logarithmic_error: 0.0377 - val_loss: 0.0255 - val_mean_squared_logarithmic_error: 0.0255\n",
            "Epoch 26/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0361 - mean_squared_logarithmic_error: 0.0361 - val_loss: 0.0246 - val_mean_squared_logarithmic_error: 0.0246\n",
            "Epoch 27/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0350 - mean_squared_logarithmic_error: 0.0350 - val_loss: 0.0241 - val_mean_squared_logarithmic_error: 0.0241\n",
            "Epoch 28/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0339 - mean_squared_logarithmic_error: 0.0339 - val_loss: 0.0234 - val_mean_squared_logarithmic_error: 0.0234\n",
            "Epoch 29/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0330 - mean_squared_logarithmic_error: 0.0330 - val_loss: 0.0229 - val_mean_squared_logarithmic_error: 0.0229\n",
            "Epoch 30/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0321 - mean_squared_logarithmic_error: 0.0321 - val_loss: 0.0225 - val_mean_squared_logarithmic_error: 0.0225\n",
            "Epoch 31/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0315 - mean_squared_logarithmic_error: 0.0315 - val_loss: 0.0221 - val_mean_squared_logarithmic_error: 0.0221\n",
            "Epoch 32/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0307 - mean_squared_logarithmic_error: 0.0307 - val_loss: 0.0219 - val_mean_squared_logarithmic_error: 0.0219\n",
            "Epoch 33/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0301 - mean_squared_logarithmic_error: 0.0301 - val_loss: 0.0220 - val_mean_squared_logarithmic_error: 0.0220\n",
            "Epoch 34/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0296 - mean_squared_logarithmic_error: 0.0296 - val_loss: 0.0216 - val_mean_squared_logarithmic_error: 0.0216\n",
            "Epoch 35/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0290 - mean_squared_logarithmic_error: 0.0290 - val_loss: 0.0214 - val_mean_squared_logarithmic_error: 0.0214\n",
            "Epoch 36/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0285 - mean_squared_logarithmic_error: 0.0285 - val_loss: 0.0213 - val_mean_squared_logarithmic_error: 0.0213\n",
            "Epoch 37/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0279 - mean_squared_logarithmic_error: 0.0279 - val_loss: 0.0211 - val_mean_squared_logarithmic_error: 0.0211\n",
            "Epoch 38/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0274 - mean_squared_logarithmic_error: 0.0274 - val_loss: 0.0207 - val_mean_squared_logarithmic_error: 0.0207\n",
            "Epoch 39/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0271 - mean_squared_logarithmic_error: 0.0271 - val_loss: 0.0206 - val_mean_squared_logarithmic_error: 0.0206\n",
            "Epoch 40/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0266 - mean_squared_logarithmic_error: 0.0266 - val_loss: 0.0199 - val_mean_squared_logarithmic_error: 0.0199\n",
            "Epoch 41/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0262 - mean_squared_logarithmic_error: 0.0262 - val_loss: 0.0198 - val_mean_squared_logarithmic_error: 0.0198\n",
            "Epoch 42/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0257 - mean_squared_logarithmic_error: 0.0257 - val_loss: 0.0196 - val_mean_squared_logarithmic_error: 0.0196\n",
            "Epoch 43/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0254 - mean_squared_logarithmic_error: 0.0254 - val_loss: 0.0194 - val_mean_squared_logarithmic_error: 0.0194\n",
            "Epoch 44/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0250 - mean_squared_logarithmic_error: 0.0250 - val_loss: 0.0197 - val_mean_squared_logarithmic_error: 0.0197\n",
            "Epoch 45/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0247 - mean_squared_logarithmic_error: 0.0247 - val_loss: 0.0195 - val_mean_squared_logarithmic_error: 0.0195\n",
            "Epoch 46/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0243 - mean_squared_logarithmic_error: 0.0243 - val_loss: 0.0193 - val_mean_squared_logarithmic_error: 0.0193\n",
            "Epoch 47/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0240 - mean_squared_logarithmic_error: 0.0240 - val_loss: 0.0191 - val_mean_squared_logarithmic_error: 0.0191\n",
            "Epoch 48/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0236 - mean_squared_logarithmic_error: 0.0236 - val_loss: 0.0193 - val_mean_squared_logarithmic_error: 0.0193\n",
            "Epoch 49/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0235 - mean_squared_logarithmic_error: 0.0235 - val_loss: 0.0194 - val_mean_squared_logarithmic_error: 0.0194\n",
            "Epoch 50/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0231 - mean_squared_logarithmic_error: 0.0231 - val_loss: 0.0187 - val_mean_squared_logarithmic_error: 0.0187\n",
            "Epoch 51/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0229 - mean_squared_logarithmic_error: 0.0229 - val_loss: 0.0188 - val_mean_squared_logarithmic_error: 0.0188\n",
            "Epoch 52/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0226 - mean_squared_logarithmic_error: 0.0226 - val_loss: 0.0192 - val_mean_squared_logarithmic_error: 0.0192\n",
            "Epoch 53/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0225 - mean_squared_logarithmic_error: 0.0225 - val_loss: 0.0192 - val_mean_squared_logarithmic_error: 0.0192\n",
            "Epoch 54/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0221 - mean_squared_logarithmic_error: 0.0221 - val_loss: 0.0185 - val_mean_squared_logarithmic_error: 0.0185\n",
            "Epoch 55/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0220 - mean_squared_logarithmic_error: 0.0220 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 56/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0217 - mean_squared_logarithmic_error: 0.0217 - val_loss: 0.0193 - val_mean_squared_logarithmic_error: 0.0193\n",
            "Epoch 57/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0214 - mean_squared_logarithmic_error: 0.0214 - val_loss: 0.0185 - val_mean_squared_logarithmic_error: 0.0185\n",
            "Epoch 58/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0212 - mean_squared_logarithmic_error: 0.0212 - val_loss: 0.0185 - val_mean_squared_logarithmic_error: 0.0185\n",
            "Epoch 59/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0211 - mean_squared_logarithmic_error: 0.0211 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 60/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0210 - mean_squared_logarithmic_error: 0.0210 - val_loss: 0.0189 - val_mean_squared_logarithmic_error: 0.0189\n",
            "Epoch 61/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0207 - mean_squared_logarithmic_error: 0.0207 - val_loss: 0.0185 - val_mean_squared_logarithmic_error: 0.0185\n",
            "Epoch 62/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0206 - mean_squared_logarithmic_error: 0.0206 - val_loss: 0.0189 - val_mean_squared_logarithmic_error: 0.0189\n",
            "Epoch 63/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0205 - mean_squared_logarithmic_error: 0.0205 - val_loss: 0.0192 - val_mean_squared_logarithmic_error: 0.0192\n",
            "Epoch 64/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0203 - mean_squared_logarithmic_error: 0.0203 - val_loss: 0.0187 - val_mean_squared_logarithmic_error: 0.0187\n",
            "Epoch 65/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0201 - mean_squared_logarithmic_error: 0.0201 - val_loss: 0.0190 - val_mean_squared_logarithmic_error: 0.0190\n",
            "Epoch 66/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0199 - mean_squared_logarithmic_error: 0.0199 - val_loss: 0.0188 - val_mean_squared_logarithmic_error: 0.0188\n",
            "Epoch 67/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0199 - mean_squared_logarithmic_error: 0.0199 - val_loss: 0.0184 - val_mean_squared_logarithmic_error: 0.0184\n",
            "Epoch 68/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0198 - mean_squared_logarithmic_error: 0.0198 - val_loss: 0.0188 - val_mean_squared_logarithmic_error: 0.0188\n",
            "Epoch 69/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0197 - mean_squared_logarithmic_error: 0.0197 - val_loss: 0.0196 - val_mean_squared_logarithmic_error: 0.0196\n",
            "Epoch 70/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0195 - mean_squared_logarithmic_error: 0.0195 - val_loss: 0.0192 - val_mean_squared_logarithmic_error: 0.0192\n",
            "Epoch 71/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0198 - mean_squared_logarithmic_error: 0.0198 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 72/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0194 - mean_squared_logarithmic_error: 0.0194 - val_loss: 0.0194 - val_mean_squared_logarithmic_error: 0.0194\n",
            "Epoch 73/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0191 - mean_squared_logarithmic_error: 0.0191 - val_loss: 0.0187 - val_mean_squared_logarithmic_error: 0.0187\n",
            "Epoch 74/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0190 - mean_squared_logarithmic_error: 0.0190 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 75/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0191 - mean_squared_logarithmic_error: 0.0191 - val_loss: 0.0189 - val_mean_squared_logarithmic_error: 0.0189\n",
            "Epoch 76/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0187 - mean_squared_logarithmic_error: 0.0187 - val_loss: 0.0185 - val_mean_squared_logarithmic_error: 0.0185\n",
            "Epoch 77/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0188 - mean_squared_logarithmic_error: 0.0188 - val_loss: 0.0188 - val_mean_squared_logarithmic_error: 0.0188\n",
            "Epoch 78/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0187 - mean_squared_logarithmic_error: 0.0187 - val_loss: 0.0190 - val_mean_squared_logarithmic_error: 0.0190\n",
            "Epoch 79/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0187 - mean_squared_logarithmic_error: 0.0187 - val_loss: 0.0187 - val_mean_squared_logarithmic_error: 0.0187\n",
            "Epoch 80/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0184 - mean_squared_logarithmic_error: 0.0184 - val_loss: 0.0188 - val_mean_squared_logarithmic_error: 0.0188\n",
            "Epoch 81/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0183 - mean_squared_logarithmic_error: 0.0183 - val_loss: 0.0191 - val_mean_squared_logarithmic_error: 0.0191\n",
            "Epoch 82/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0183 - mean_squared_logarithmic_error: 0.0183 - val_loss: 0.0187 - val_mean_squared_logarithmic_error: 0.0187\n",
            "Epoch 83/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0181 - mean_squared_logarithmic_error: 0.0181 - val_loss: 0.0182 - val_mean_squared_logarithmic_error: 0.0182\n",
            "Epoch 84/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0182 - mean_squared_logarithmic_error: 0.0182 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 85/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0180 - mean_squared_logarithmic_error: 0.0180 - val_loss: 0.0189 - val_mean_squared_logarithmic_error: 0.0189\n",
            "Epoch 86/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0179 - mean_squared_logarithmic_error: 0.0179 - val_loss: 0.0192 - val_mean_squared_logarithmic_error: 0.0192\n",
            "Epoch 87/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0178 - mean_squared_logarithmic_error: 0.0178 - val_loss: 0.0188 - val_mean_squared_logarithmic_error: 0.0188\n",
            "Epoch 88/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0177 - mean_squared_logarithmic_error: 0.0177 - val_loss: 0.0189 - val_mean_squared_logarithmic_error: 0.0189\n",
            "Epoch 89/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0177 - mean_squared_logarithmic_error: 0.0177 - val_loss: 0.0191 - val_mean_squared_logarithmic_error: 0.0191\n",
            "Epoch 90/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0178 - mean_squared_logarithmic_error: 0.0178 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 91/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0177 - mean_squared_logarithmic_error: 0.0177 - val_loss: 0.0194 - val_mean_squared_logarithmic_error: 0.0194\n",
            "Epoch 92/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0175 - mean_squared_logarithmic_error: 0.0175 - val_loss: 0.0187 - val_mean_squared_logarithmic_error: 0.0187\n",
            "Epoch 93/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0174 - mean_squared_logarithmic_error: 0.0174 - val_loss: 0.0188 - val_mean_squared_logarithmic_error: 0.0188\n",
            "Epoch 94/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0172 - mean_squared_logarithmic_error: 0.0172 - val_loss: 0.0188 - val_mean_squared_logarithmic_error: 0.0188\n",
            "Epoch 95/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0172 - mean_squared_logarithmic_error: 0.0172 - val_loss: 0.0191 - val_mean_squared_logarithmic_error: 0.0191\n",
            "Epoch 96/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0171 - mean_squared_logarithmic_error: 0.0171 - val_loss: 0.0188 - val_mean_squared_logarithmic_error: 0.0188\n",
            "Epoch 97/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0170 - mean_squared_logarithmic_error: 0.0170 - val_loss: 0.0188 - val_mean_squared_logarithmic_error: 0.0188\n",
            "Epoch 98/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0170 - mean_squared_logarithmic_error: 0.0170 - val_loss: 0.0187 - val_mean_squared_logarithmic_error: 0.0187\n",
            "Epoch 99/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0170 - mean_squared_logarithmic_error: 0.0170 - val_loss: 0.0188 - val_mean_squared_logarithmic_error: 0.0188\n",
            "Epoch 100/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0168 - mean_squared_logarithmic_error: 0.0168 - val_loss: 0.0190 - val_mean_squared_logarithmic_error: 0.0190\n",
            "Epoch 101/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0168 - mean_squared_logarithmic_error: 0.0168 - val_loss: 0.0191 - val_mean_squared_logarithmic_error: 0.0191\n",
            "Epoch 102/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0167 - mean_squared_logarithmic_error: 0.0167 - val_loss: 0.0184 - val_mean_squared_logarithmic_error: 0.0184\n",
            "Epoch 103/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0167 - mean_squared_logarithmic_error: 0.0167 - val_loss: 0.0187 - val_mean_squared_logarithmic_error: 0.0187\n",
            "Epoch 104/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0166 - mean_squared_logarithmic_error: 0.0166 - val_loss: 0.0184 - val_mean_squared_logarithmic_error: 0.0184\n",
            "Epoch 105/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0167 - mean_squared_logarithmic_error: 0.0167 - val_loss: 0.0195 - val_mean_squared_logarithmic_error: 0.0195\n",
            "Epoch 106/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0170 - mean_squared_logarithmic_error: 0.0170 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 107/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0167 - mean_squared_logarithmic_error: 0.0167 - val_loss: 0.0188 - val_mean_squared_logarithmic_error: 0.0188\n",
            "Epoch 108/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0165 - mean_squared_logarithmic_error: 0.0165 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 109/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0164 - mean_squared_logarithmic_error: 0.0164 - val_loss: 0.0181 - val_mean_squared_logarithmic_error: 0.0181\n",
            "Epoch 110/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0164 - mean_squared_logarithmic_error: 0.0164 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 111/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0164 - mean_squared_logarithmic_error: 0.0164 - val_loss: 0.0185 - val_mean_squared_logarithmic_error: 0.0185\n",
            "Epoch 112/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0163 - mean_squared_logarithmic_error: 0.0163 - val_loss: 0.0189 - val_mean_squared_logarithmic_error: 0.0189\n",
            "Epoch 113/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0162 - mean_squared_logarithmic_error: 0.0162 - val_loss: 0.0187 - val_mean_squared_logarithmic_error: 0.0187\n",
            "Epoch 114/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0161 - mean_squared_logarithmic_error: 0.0161 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 115/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0160 - mean_squared_logarithmic_error: 0.0160 - val_loss: 0.0185 - val_mean_squared_logarithmic_error: 0.0185\n",
            "Epoch 116/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0160 - mean_squared_logarithmic_error: 0.0160 - val_loss: 0.0179 - val_mean_squared_logarithmic_error: 0.0179\n",
            "Epoch 117/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0158 - mean_squared_logarithmic_error: 0.0158 - val_loss: 0.0184 - val_mean_squared_logarithmic_error: 0.0184\n",
            "Epoch 118/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0158 - mean_squared_logarithmic_error: 0.0158 - val_loss: 0.0185 - val_mean_squared_logarithmic_error: 0.0185\n",
            "Epoch 119/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0158 - mean_squared_logarithmic_error: 0.0158 - val_loss: 0.0185 - val_mean_squared_logarithmic_error: 0.0185\n",
            "Epoch 120/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0157 - mean_squared_logarithmic_error: 0.0157 - val_loss: 0.0182 - val_mean_squared_logarithmic_error: 0.0182\n",
            "Epoch 121/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0156 - mean_squared_logarithmic_error: 0.0156 - val_loss: 0.0188 - val_mean_squared_logarithmic_error: 0.0188\n",
            "Epoch 122/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0156 - mean_squared_logarithmic_error: 0.0156 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 123/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0154 - mean_squared_logarithmic_error: 0.0154 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 124/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0155 - mean_squared_logarithmic_error: 0.0155 - val_loss: 0.0181 - val_mean_squared_logarithmic_error: 0.0181\n",
            "Epoch 125/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0155 - mean_squared_logarithmic_error: 0.0155 - val_loss: 0.0184 - val_mean_squared_logarithmic_error: 0.0184\n",
            "Epoch 126/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0156 - mean_squared_logarithmic_error: 0.0156 - val_loss: 0.0180 - val_mean_squared_logarithmic_error: 0.0180\n",
            "Epoch 127/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0154 - mean_squared_logarithmic_error: 0.0154 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 128/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0157 - mean_squared_logarithmic_error: 0.0157 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 129/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0151 - mean_squared_logarithmic_error: 0.0151 - val_loss: 0.0174 - val_mean_squared_logarithmic_error: 0.0174\n",
            "Epoch 130/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0153 - mean_squared_logarithmic_error: 0.0153 - val_loss: 0.0177 - val_mean_squared_logarithmic_error: 0.0177\n",
            "Epoch 131/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0150 - mean_squared_logarithmic_error: 0.0150 - val_loss: 0.0189 - val_mean_squared_logarithmic_error: 0.0189\n",
            "Epoch 132/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0152 - mean_squared_logarithmic_error: 0.0152 - val_loss: 0.0174 - val_mean_squared_logarithmic_error: 0.0174\n",
            "Epoch 133/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0148 - mean_squared_logarithmic_error: 0.0148 - val_loss: 0.0177 - val_mean_squared_logarithmic_error: 0.0177\n",
            "Epoch 134/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0152 - mean_squared_logarithmic_error: 0.0152 - val_loss: 0.0174 - val_mean_squared_logarithmic_error: 0.0174\n",
            "Epoch 135/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0150 - mean_squared_logarithmic_error: 0.0150 - val_loss: 0.0182 - val_mean_squared_logarithmic_error: 0.0182\n",
            "Epoch 136/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0148 - mean_squared_logarithmic_error: 0.0148 - val_loss: 0.0173 - val_mean_squared_logarithmic_error: 0.0173\n",
            "Epoch 137/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0148 - mean_squared_logarithmic_error: 0.0148 - val_loss: 0.0176 - val_mean_squared_logarithmic_error: 0.0176\n",
            "Epoch 138/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0147 - mean_squared_logarithmic_error: 0.0147 - val_loss: 0.0177 - val_mean_squared_logarithmic_error: 0.0177\n",
            "Epoch 139/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0146 - mean_squared_logarithmic_error: 0.0146 - val_loss: 0.0173 - val_mean_squared_logarithmic_error: 0.0173\n",
            "Epoch 140/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0145 - mean_squared_logarithmic_error: 0.0145 - val_loss: 0.0175 - val_mean_squared_logarithmic_error: 0.0175\n",
            "Epoch 141/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0147 - mean_squared_logarithmic_error: 0.0147 - val_loss: 0.0176 - val_mean_squared_logarithmic_error: 0.0176\n",
            "Epoch 142/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0146 - mean_squared_logarithmic_error: 0.0146 - val_loss: 0.0171 - val_mean_squared_logarithmic_error: 0.0171\n",
            "Epoch 143/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0143 - mean_squared_logarithmic_error: 0.0143 - val_loss: 0.0177 - val_mean_squared_logarithmic_error: 0.0177\n",
            "Epoch 144/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0145 - mean_squared_logarithmic_error: 0.0145 - val_loss: 0.0171 - val_mean_squared_logarithmic_error: 0.0171\n",
            "Epoch 145/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0142 - mean_squared_logarithmic_error: 0.0142 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 146/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0149 - mean_squared_logarithmic_error: 0.0149 - val_loss: 0.0171 - val_mean_squared_logarithmic_error: 0.0171\n",
            "Epoch 147/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0142 - mean_squared_logarithmic_error: 0.0142 - val_loss: 0.0172 - val_mean_squared_logarithmic_error: 0.0172\n",
            "Epoch 148/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0141 - mean_squared_logarithmic_error: 0.0141 - val_loss: 0.0173 - val_mean_squared_logarithmic_error: 0.0173\n",
            "Epoch 149/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0140 - mean_squared_logarithmic_error: 0.0140 - val_loss: 0.0171 - val_mean_squared_logarithmic_error: 0.0171\n",
            "Epoch 150/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0141 - mean_squared_logarithmic_error: 0.0141 - val_loss: 0.0169 - val_mean_squared_logarithmic_error: 0.0169\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb4910c8250>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc_jN5XwcBu9"
      },
      "source": [
        "## Question 3\n",
        "\n",
        "Now that you know how MSLE works, you need to plot the behavior of MSLE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_USeo3P7cBu-"
      },
      "source": [
        "### Answer 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHr_XxgUcBu-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "8b8c134f-6e61-4105-e15d-4d6d4bd6e9d0"
      },
      "source": [
        "actual_outputs = np.arange(0, 51)\n",
        "n = len(actual_outputs)\n",
        "estimated_outputs = np.zeros(n)\n",
        "\n",
        "msle = np.square(np.log(actual_outputs + 1) - np.log(estimated_outputs + 1))/n\n",
        "\n",
        "plt.plot(actual_outputs, msle, c='#F47789', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Actual ouputs')\n",
        "plt.ylabel('Mean Squared Logarithmic Errors')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BylzTZ1W9Ma"
      },
      "source": [
        "## Question 4\n",
        "\n",
        "Why do we add $1$ to the outputs before passing it through $\\log()$? \n",
        "\n",
        "## Answer 4\n",
        "If we did not add 1 to the outputs, we would have encountered a divide by zero error in the log function. In other words, log(0) is undefined. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaQiO_XeYTjE"
      },
      "source": [
        "## Question 5\n",
        "\n",
        "Write your observations about MSE, MAE, and MSLE; and compare the results achieved with all 3 loss functions. \n",
        "\n",
        "## Answer 5\n",
        "\n",
        "MSE: ***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efp4KP5GfDL7"
      },
      "source": [
        "## Question 6\n",
        "\n",
        "Plug-in any of the loss functions from [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/losses) docs to the `model.compile` method and see if the difference in model performance as compared to MSE, MAE, and MSLE.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDWlkms1flkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef079e0c-a94c-4d49-be82-dbaa4ee9fd4e"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss= tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=[tf.keras.metrics.BinaryCrossentropy()])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 2s 17ms/step - loss: -6.0323 - binary_crossentropy: 54.5050 - val_loss: -15.7028 - val_binary_crossentropy: -84.0984\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -26.6320 - binary_crossentropy: -209.4069 - val_loss: -40.4394 - val_binary_crossentropy: -312.9070\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -64.8673 - binary_crossentropy: -325.8416 - val_loss: -92.7093 - val_binary_crossentropy: -312.9070\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -143.3610 - binary_crossentropy: -327.7662 - val_loss: -202.5723 - val_binary_crossentropy: -312.9070\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -308.1993 - binary_crossentropy: -327.7662 - val_loss: -417.9430 - val_binary_crossentropy: -312.9070\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -618.2412 - binary_crossentropy: -327.7662 - val_loss: -820.2675 - val_binary_crossentropy: -312.9070\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1181.9354 - binary_crossentropy: -327.7662 - val_loss: -1516.2365 - val_binary_crossentropy: -312.9070\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -2120.3301 - binary_crossentropy: -327.7662 - val_loss: -2648.0535 - val_binary_crossentropy: -312.9070\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -3591.4373 - binary_crossentropy: -327.7662 - val_loss: -4435.4819 - val_binary_crossentropy: -312.9070\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -5919.2935 - binary_crossentropy: -327.7662 - val_loss: -7102.4727 - val_binary_crossentropy: -312.9070\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -9264.6416 - binary_crossentropy: -327.7662 - val_loss: -10962.0693 - val_binary_crossentropy: -312.9070\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -14120.0879 - binary_crossentropy: -327.7662 - val_loss: -16296.3418 - val_binary_crossentropy: -312.9070\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -20887.5117 - binary_crossentropy: -327.7662 - val_loss: -23403.7734 - val_binary_crossentropy: -312.9070\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -29450.5996 - binary_crossentropy: -327.7662 - val_loss: -33076.4062 - val_binary_crossentropy: -312.9070\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -41141.3359 - binary_crossentropy: -327.7662 - val_loss: -45351.7422 - val_binary_crossentropy: -312.9070\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -55941.3594 - binary_crossentropy: -327.7662 - val_loss: -60930.4258 - val_binary_crossentropy: -312.9070\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -74574.0312 - binary_crossentropy: -327.7662 - val_loss: -80556.0234 - val_binary_crossentropy: -312.9070\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -97820.1016 - binary_crossentropy: -327.7662 - val_loss: -104746.1484 - val_binary_crossentropy: -312.9070\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -126097.4219 - binary_crossentropy: -327.7662 - val_loss: -134127.1250 - val_binary_crossentropy: -312.9070\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -160601.7344 - binary_crossentropy: -327.7662 - val_loss: -168966.7500 - val_binary_crossentropy: -312.9070\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -202370.5781 - binary_crossentropy: -327.7662 - val_loss: -210239.8594 - val_binary_crossentropy: -312.9070\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -250023.5156 - binary_crossentropy: -327.7662 - val_loss: -259833.3594 - val_binary_crossentropy: -312.9070\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -307201.2500 - binary_crossentropy: -327.7662 - val_loss: -315784.8125 - val_binary_crossentropy: -312.9070\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -371519.1250 - binary_crossentropy: -327.7662 - val_loss: -381213.7188 - val_binary_crossentropy: -312.9070\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -446130.7812 - binary_crossentropy: -327.7662 - val_loss: -455970.3438 - val_binary_crossentropy: -312.9070\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -529875.0000 - binary_crossentropy: -327.7662 - val_loss: -541526.4375 - val_binary_crossentropy: -312.9070\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -628761.3125 - binary_crossentropy: -327.7662 - val_loss: -637141.7500 - val_binary_crossentropy: -312.9070\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -740339.3125 - binary_crossentropy: -327.7662 - val_loss: -747647.1875 - val_binary_crossentropy: -312.9070\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -863670.3125 - binary_crossentropy: -327.7662 - val_loss: -872508.3125 - val_binary_crossentropy: -312.9070\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -1003535.9375 - binary_crossentropy: -327.7662 - val_loss: -1008367.4375 - val_binary_crossentropy: -312.9070\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1157129.6250 - binary_crossentropy: -327.7662 - val_loss: -1159462.5000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -1326782.8750 - binary_crossentropy: -327.7662 - val_loss: -1326796.6250 - val_binary_crossentropy: -312.9070\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1516242.7500 - binary_crossentropy: -327.7662 - val_loss: -1511379.1250 - val_binary_crossentropy: -312.9070\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1724050.7500 - binary_crossentropy: -327.7662 - val_loss: -1715591.7500 - val_binary_crossentropy: -312.9070\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1958989.6250 - binary_crossentropy: -327.7662 - val_loss: -1939592.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -2213286.0000 - binary_crossentropy: -327.7662 - val_loss: -2185973.7500 - val_binary_crossentropy: -312.9070\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -2491226.5000 - binary_crossentropy: -327.7662 - val_loss: -2458696.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -2788335.7500 - binary_crossentropy: -327.7662 - val_loss: -2763373.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -3131217.0000 - binary_crossentropy: -327.7662 - val_loss: -3080964.7500 - val_binary_crossentropy: -312.9070\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -3481379.2500 - binary_crossentropy: -327.7662 - val_loss: -3424917.7500 - val_binary_crossentropy: -312.9070\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: -3866283.0000 - binary_crossentropy: -327.7662 - val_loss: -3798705.2500 - val_binary_crossentropy: -312.9070\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -4279173.0000 - binary_crossentropy: -327.7662 - val_loss: -4200139.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -4726481.0000 - binary_crossentropy: -327.7662 - val_loss: -4630418.5000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -5205924.0000 - binary_crossentropy: -327.7662 - val_loss: -5093050.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -5722737.0000 - binary_crossentropy: -327.7662 - val_loss: -5585208.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -6274096.0000 - binary_crossentropy: -327.7662 - val_loss: -6108911.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -6860853.0000 - binary_crossentropy: -327.7662 - val_loss: -6669675.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -7482679.5000 - binary_crossentropy: -327.7662 - val_loss: -7283794.5000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -8148033.0000 - binary_crossentropy: -327.7662 - val_loss: -7928026.5000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -8867083.0000 - binary_crossentropy: -327.7662 - val_loss: -8597652.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -9626527.0000 - binary_crossentropy: -327.7662 - val_loss: -9294910.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -10406146.0000 - binary_crossentropy: -327.7662 - val_loss: -10068884.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -11252782.0000 - binary_crossentropy: -327.7662 - val_loss: -10888882.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -12174024.0000 - binary_crossentropy: -327.7662 - val_loss: -11745233.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -13105572.0000 - binary_crossentropy: -327.7662 - val_loss: -12653329.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -14124969.0000 - binary_crossentropy: -327.7662 - val_loss: -13586065.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -15135946.0000 - binary_crossentropy: -327.7662 - val_loss: -14605371.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -16276586.0000 - binary_crossentropy: -327.7662 - val_loss: -15623607.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -17393054.0000 - binary_crossentropy: -327.7662 - val_loss: -16756325.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -18636140.0000 - binary_crossentropy: -327.7662 - val_loss: -17923066.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -19938708.0000 - binary_crossentropy: -327.7662 - val_loss: -19145844.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -21288648.0000 - binary_crossentropy: -327.7662 - val_loss: -20416272.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -22678682.0000 - binary_crossentropy: -327.7662 - val_loss: -21772682.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -24183140.0000 - binary_crossentropy: -327.7662 - val_loss: -23169696.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -25713960.0000 - binary_crossentropy: -327.7662 - val_loss: -24662444.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -27364166.0000 - binary_crossentropy: -327.7662 - val_loss: -26182406.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -29046502.0000 - binary_crossentropy: -327.7662 - val_loss: -27798552.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: -30793144.0000 - binary_crossentropy: -327.7662 - val_loss: -29481628.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -32670034.0000 - binary_crossentropy: -327.7662 - val_loss: -31217214.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -34574256.0000 - binary_crossentropy: -327.7662 - val_loss: -33054764.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -36610828.0000 - binary_crossentropy: -327.7662 - val_loss: -34922988.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -38651392.0000 - binary_crossentropy: -327.7662 - val_loss: -36914888.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -40817940.0000 - binary_crossentropy: -327.7662 - val_loss: -39004556.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -43098440.0000 - binary_crossentropy: -327.7662 - val_loss: -41076884.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -45390612.0000 - binary_crossentropy: -327.7662 - val_loss: -43283936.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -47815024.0000 - binary_crossentropy: -327.7662 - val_loss: -45595924.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -50364832.0000 - binary_crossentropy: -327.7662 - val_loss: -47945424.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -53010936.0000 - binary_crossentropy: -327.7662 - val_loss: -50453808.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -55742700.0000 - binary_crossentropy: -327.7662 - val_loss: -53094676.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -58620932.0000 - binary_crossentropy: -327.7662 - val_loss: -55775992.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -61603444.0000 - binary_crossentropy: -327.7662 - val_loss: -58525548.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -64545424.0000 - binary_crossentropy: -327.7662 - val_loss: -61347392.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -67610904.0000 - binary_crossentropy: -327.7662 - val_loss: -64214428.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -70643184.0000 - binary_crossentropy: -327.7662 - val_loss: -67240672.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -73953504.0000 - binary_crossentropy: -327.7662 - val_loss: -70208272.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -77219064.0000 - binary_crossentropy: -327.7662 - val_loss: -73313528.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -80684696.0000 - binary_crossentropy: -327.7662 - val_loss: -76509744.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -84143400.0000 - binary_crossentropy: -327.7662 - val_loss: -79927392.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -87947064.0000 - binary_crossentropy: -327.7662 - val_loss: -83319248.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -91718232.0000 - binary_crossentropy: -327.7662 - val_loss: -87054880.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -95595144.0000 - binary_crossentropy: -327.7662 - val_loss: -90825104.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -99652400.0000 - binary_crossentropy: -327.7662 - val_loss: -94545632.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -103718328.0000 - binary_crossentropy: -327.7662 - val_loss: -98315816.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -107918792.0000 - binary_crossentropy: -327.7662 - val_loss: -102189648.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -112133032.0000 - binary_crossentropy: -327.7662 - val_loss: -106331496.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -116661856.0000 - binary_crossentropy: -327.7662 - val_loss: -110521680.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -121299744.0000 - binary_crossentropy: -327.7662 - val_loss: -114898992.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -126113760.0000 - binary_crossentropy: -327.7662 - val_loss: -119528312.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -131157032.0000 - binary_crossentropy: -327.7662 - val_loss: -124122040.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -136157664.0000 - binary_crossentropy: -327.7662 - val_loss: -128802528.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -141166320.0000 - binary_crossentropy: -327.7662 - val_loss: -133672624.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -146479312.0000 - binary_crossentropy: -327.7662 - val_loss: -138550592.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -151894704.0000 - binary_crossentropy: -327.7662 - val_loss: -143546960.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: -157309504.0000 - binary_crossentropy: -327.7662 - val_loss: -148836272.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -163048672.0000 - binary_crossentropy: -327.7662 - val_loss: -154213632.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: -168905376.0000 - binary_crossentropy: -327.7662 - val_loss: -159683888.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -174779904.0000 - binary_crossentropy: -327.7662 - val_loss: -165279936.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -180959760.0000 - binary_crossentropy: -327.7662 - val_loss: -171059072.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -187192752.0000 - binary_crossentropy: -327.7662 - val_loss: -176866640.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -193489760.0000 - binary_crossentropy: -327.7662 - val_loss: -182855856.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -200105776.0000 - binary_crossentropy: -327.7662 - val_loss: -188869856.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -206788784.0000 - binary_crossentropy: -327.7662 - val_loss: -195287280.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -213750080.0000 - binary_crossentropy: -327.7662 - val_loss: -201858496.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -220863504.0000 - binary_crossentropy: -327.7662 - val_loss: -208418432.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -228084208.0000 - binary_crossentropy: -327.7662 - val_loss: -215251264.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -235449232.0000 - binary_crossentropy: -327.7662 - val_loss: -222128576.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -242801584.0000 - binary_crossentropy: -327.7662 - val_loss: -229320352.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -250728752.0000 - binary_crossentropy: -327.7662 - val_loss: -236462784.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -258546608.0000 - binary_crossentropy: -327.7662 - val_loss: -243807328.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -266509728.0000 - binary_crossentropy: -327.7662 - val_loss: -251302432.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -274477280.0000 - binary_crossentropy: -327.7662 - val_loss: -259156512.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -283093504.0000 - binary_crossentropy: -327.7662 - val_loss: -266870336.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -291550400.0000 - binary_crossentropy: -327.7662 - val_loss: -274787808.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -300231424.0000 - binary_crossentropy: -327.7662 - val_loss: -282894976.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -309205024.0000 - binary_crossentropy: -327.7662 - val_loss: -291271168.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -318040640.0000 - binary_crossentropy: -327.7662 - val_loss: -300142752.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -327740032.0000 - binary_crossentropy: -327.7662 - val_loss: -308748192.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -337098464.0000 - binary_crossentropy: -327.7662 - val_loss: -317675680.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -346832000.0000 - binary_crossentropy: -327.7662 - val_loss: -326745728.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -356665184.0000 - binary_crossentropy: -327.7662 - val_loss: -336053184.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -366906528.0000 - binary_crossentropy: -327.7662 - val_loss: -345516832.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -377172896.0000 - binary_crossentropy: -327.7662 - val_loss: -355206560.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -387654880.0000 - binary_crossentropy: -327.7662 - val_loss: -364864544.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -398258624.0000 - binary_crossentropy: -327.7662 - val_loss: -375077824.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -409243616.0000 - binary_crossentropy: -327.7662 - val_loss: -385313024.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -420160544.0000 - binary_crossentropy: -327.7662 - val_loss: -395760992.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -431412608.0000 - binary_crossentropy: -327.7662 - val_loss: -406113216.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -442847968.0000 - binary_crossentropy: -327.7662 - val_loss: -416628096.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -454518368.0000 - binary_crossentropy: -327.7662 - val_loss: -427410464.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -466242688.0000 - binary_crossentropy: -327.7662 - val_loss: -438727456.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -478194560.0000 - binary_crossentropy: -327.7662 - val_loss: -450081664.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -490491840.0000 - binary_crossentropy: -327.7662 - val_loss: -461471040.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -502967456.0000 - binary_crossentropy: -327.7662 - val_loss: -472938304.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -515529568.0000 - binary_crossentropy: -327.7662 - val_loss: -484656800.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -528013344.0000 - binary_crossentropy: -327.7662 - val_loss: -496992256.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: -541606336.0000 - binary_crossentropy: -327.7662 - val_loss: -508980544.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -554859200.0000 - binary_crossentropy: -327.7662 - val_loss: -521528448.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -568606848.0000 - binary_crossentropy: -327.7662 - val_loss: -534376608.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -582375040.0000 - binary_crossentropy: -327.7662 - val_loss: -547325888.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -596293120.0000 - binary_crossentropy: -327.7662 - val_loss: -560628288.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -610706176.0000 - binary_crossentropy: -327.7662 - val_loss: -573699648.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -624960448.0000 - binary_crossentropy: -327.7662 - val_loss: -587239040.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -639583808.0000 - binary_crossentropy: -327.7662 - val_loss: -601076736.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -654690304.0000 - binary_crossentropy: -327.7662 - val_loss: -614605184.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -669020992.0000 - binary_crossentropy: -327.7662 - val_loss: -628778944.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -684914816.0000 - binary_crossentropy: -327.7662 - val_loss: -642809280.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -700179904.0000 - binary_crossentropy: -327.7662 - val_loss: -657741824.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -716564800.0000 - binary_crossentropy: -327.7662 - val_loss: -672898048.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -732702016.0000 - binary_crossentropy: -327.7662 - val_loss: -688183040.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -749103680.0000 - binary_crossentropy: -327.7662 - val_loss: -703539904.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -765793408.0000 - binary_crossentropy: -327.7662 - val_loss: -719350592.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -783117824.0000 - binary_crossentropy: -327.7662 - val_loss: -734999424.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -799848704.0000 - binary_crossentropy: -327.7662 - val_loss: -751100096.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -817349440.0000 - binary_crossentropy: -327.7662 - val_loss: -767365888.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -834918528.0000 - binary_crossentropy: -327.7662 - val_loss: -783543104.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -852427648.0000 - binary_crossentropy: -327.7662 - val_loss: -800352448.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -870489024.0000 - binary_crossentropy: -327.7662 - val_loss: -817078976.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -888828928.0000 - binary_crossentropy: -327.7662 - val_loss: -833891584.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -907415232.0000 - binary_crossentropy: -327.7662 - val_loss: -851393472.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -926425088.0000 - binary_crossentropy: -327.7662 - val_loss: -869159936.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -945048896.0000 - binary_crossentropy: -327.7662 - val_loss: -887324544.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -964965248.0000 - binary_crossentropy: -327.7662 - val_loss: -904944960.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -984504448.0000 - binary_crossentropy: -327.7662 - val_loss: -923081024.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1004222144.0000 - binary_crossentropy: -327.7662 - val_loss: -941866240.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1024463744.0000 - binary_crossentropy: -327.7662 - val_loss: -961189760.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -1045291328.0000 - binary_crossentropy: -327.7662 - val_loss: -980436032.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1066239040.0000 - binary_crossentropy: -327.7662 - val_loss: -999980864.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1087818112.0000 - binary_crossentropy: -327.7662 - val_loss: -1020078016.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -1109343488.0000 - binary_crossentropy: -327.7662 - val_loss: -1040285312.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1131290880.0000 - binary_crossentropy: -327.7662 - val_loss: -1060563904.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1153033856.0000 - binary_crossentropy: -327.7662 - val_loss: -1081484416.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1175559552.0000 - binary_crossentropy: -327.7662 - val_loss: -1102689792.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1198578560.0000 - binary_crossentropy: -327.7662 - val_loss: -1123756544.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1221974400.0000 - binary_crossentropy: -327.7662 - val_loss: -1144806784.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1244868864.0000 - binary_crossentropy: -327.7662 - val_loss: -1167331968.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1268970368.0000 - binary_crossentropy: -327.7662 - val_loss: -1189174528.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1292752256.0000 - binary_crossentropy: -327.7662 - val_loss: -1211049984.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -1316604928.0000 - binary_crossentropy: -327.7662 - val_loss: -1233192704.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1340574720.0000 - binary_crossentropy: -327.7662 - val_loss: -1256443776.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1365254784.0000 - binary_crossentropy: -327.7662 - val_loss: -1279566080.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -1390278016.0000 - binary_crossentropy: -327.7662 - val_loss: -1302319616.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1414856192.0000 - binary_crossentropy: -327.7662 - val_loss: -1325624192.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1440094848.0000 - binary_crossentropy: -327.7662 - val_loss: -1349146240.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1465740544.0000 - binary_crossentropy: -327.7662 - val_loss: -1373140096.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1492135168.0000 - binary_crossentropy: -327.7662 - val_loss: -1396748032.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1517483392.0000 - binary_crossentropy: -327.7662 - val_loss: -1421828736.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1544699776.0000 - binary_crossentropy: -327.7662 - val_loss: -1446934144.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1572144384.0000 - binary_crossentropy: -327.7662 - val_loss: -1472187520.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -1599414144.0000 - binary_crossentropy: -327.7662 - val_loss: -1498042496.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1627511936.0000 - binary_crossentropy: -327.7662 - val_loss: -1524387584.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1656062464.0000 - binary_crossentropy: -327.7662 - val_loss: -1550408832.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1683835392.0000 - binary_crossentropy: -327.7662 - val_loss: -1576787328.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1712608896.0000 - binary_crossentropy: -327.7662 - val_loss: -1602982272.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1741442432.0000 - binary_crossentropy: -327.7662 - val_loss: -1630628736.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1771292544.0000 - binary_crossentropy: -327.7662 - val_loss: -1658286208.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1801083520.0000 - binary_crossentropy: -327.7662 - val_loss: -1685659264.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1830993536.0000 - binary_crossentropy: -327.7662 - val_loss: -1713812608.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1861625856.0000 - binary_crossentropy: -327.7662 - val_loss: -1742375424.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1892276352.0000 - binary_crossentropy: -327.7662 - val_loss: -1771369728.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1923534848.0000 - binary_crossentropy: -327.7662 - val_loss: -1800486272.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -1954048128.0000 - binary_crossentropy: -327.7662 - val_loss: -1830131584.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -1986603264.0000 - binary_crossentropy: -327.7662 - val_loss: -1858905600.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -2018474368.0000 - binary_crossentropy: -327.7662 - val_loss: -1888375936.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -2050108160.0000 - binary_crossentropy: -327.7662 - val_loss: -1918487040.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -2082932864.0000 - binary_crossentropy: -327.7662 - val_loss: -1949000448.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -2115705856.0000 - binary_crossentropy: -327.7662 - val_loss: -1979437312.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -2149412608.0000 - binary_crossentropy: -327.7662 - val_loss: -2009929600.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -2182544896.0000 - binary_crossentropy: -327.7662 - val_loss: -2042319744.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -2217288448.0000 - binary_crossentropy: -327.7662 - val_loss: -2074623232.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -2251917824.0000 - binary_crossentropy: -327.7662 - val_loss: -2106668672.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -2286442496.0000 - binary_crossentropy: -327.7662 - val_loss: -2139038720.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -2321205248.0000 - binary_crossentropy: -327.7662 - val_loss: -2171718400.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -2357296128.0000 - binary_crossentropy: -327.7662 - val_loss: -2205049344.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -2392947456.0000 - binary_crossentropy: -327.7662 - val_loss: -2238324480.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -2428872192.0000 - binary_crossentropy: -327.7662 - val_loss: -2272062464.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -2466067712.0000 - binary_crossentropy: -327.7662 - val_loss: -2306104064.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -2502416128.0000 - binary_crossentropy: -327.7662 - val_loss: -2340701440.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -2540286976.0000 - binary_crossentropy: -327.7662 - val_loss: -2375034112.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -2577579008.0000 - binary_crossentropy: -327.7662 - val_loss: -2410734848.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: -2615113984.0000 - binary_crossentropy: -327.7662 - val_loss: -2446521344.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -2654193920.0000 - binary_crossentropy: -327.7662 - val_loss: -2481671424.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -2692406784.0000 - binary_crossentropy: -327.7662 - val_loss: -2516965888.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -2729880832.0000 - binary_crossentropy: -327.7662 - val_loss: -2553446656.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -2769659648.0000 - binary_crossentropy: -327.7662 - val_loss: -2589764864.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -2809513216.0000 - binary_crossentropy: -327.7662 - val_loss: -2626786048.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -2849271040.0000 - binary_crossentropy: -327.7662 - val_loss: -2664654080.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -2890746624.0000 - binary_crossentropy: -327.7662 - val_loss: -2702266624.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -2931213824.0000 - binary_crossentropy: -327.7662 - val_loss: -2740756992.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -2973303808.0000 - binary_crossentropy: -327.7662 - val_loss: -2778661120.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -3014024192.0000 - binary_crossentropy: -327.7662 - val_loss: -2817490176.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: -3055702784.0000 - binary_crossentropy: -327.7662 - val_loss: -2858073088.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -3099033600.0000 - binary_crossentropy: -327.7662 - val_loss: -2897471232.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: -3143408128.0000 - binary_crossentropy: -327.7662 - val_loss: -2936317440.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -3185127936.0000 - binary_crossentropy: -327.7662 - val_loss: -2978797312.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -3230667776.0000 - binary_crossentropy: -327.7662 - val_loss: -3019157504.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -3274262272.0000 - binary_crossentropy: -327.7662 - val_loss: -3060293376.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: -3318081536.0000 - binary_crossentropy: -327.7662 - val_loss: -3102086144.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -3363798272.0000 - binary_crossentropy: -327.7662 - val_loss: -3142847744.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -3407747840.0000 - binary_crossentropy: -327.7662 - val_loss: -3185040896.0000 - val_binary_crossentropy: -312.9070\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: -3453868288.0000 - binary_crossentropy: -327.7662 - val_loss: -3226572288.0000 - val_binary_crossentropy: -312.9070\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5eaf79a090>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mupD9JvzD1BU"
      },
      "source": [
        "#Fun Fact\n",
        "\n",
        "Google Translate is getting better all the time, but it's still not perfect. Translate a sentence into another language and back into English, and you might get a hilarious surprise. That's what Malinda Kathleen Reese got when she reverse Google Translated the lyrics to \"Let It Go\" from Disney's Frozen into Chinese, Macedonian, French, Polish, Creole, Tamil and others. It doesn't come out as utter gibberish, but as a slightly off version with a slightly different message from the original. Which makes it even funnier. Plus, Malinda can really sing.\n",
        "\n",
        "Link to video: https://www.youtube.com/watch?v=2bVAoVlFYf0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t0A5Xui7sum"
      },
      "source": [
        "# Classification Losses\n",
        "\n",
        "In classification, the outputs are in form of a class or a category. The label or number assigned to the classes do not have a numerical meaning. \n",
        "\n",
        "For example, an input with class label 0 cannot be numerically compared with an input with class label 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERGOjkbwjCT1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "549f2d64-b458-46ee-c4bb-37ea0e9f953f"
      },
      "source": [
        "(training_images, training_labels), (test_images, test_labels) = tf.keras.datasets.cifar100.load_data(label_mode=\"coarse\")\n",
        "\n",
        "training_images=training_images.reshape(50000, 32, 32, 3)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 32, 32, 3)\n",
        "test_images=test_images/255.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 6s 0us/step\n",
            "169017344/169001437 [==============================] - 6s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EbtSlPYZYju"
      },
      "source": [
        "## Kullback-Leibler Divergence [KDL]\n",
        "\n",
        "Kullback Leibler Divergence Loss is a measure of how a distribution varies from a reference distribution (or a baseline distribution). A Kullback Leibler Divergence Loss of zero means that both the probability distributions are identical.\n",
        "\n",
        "The number of information lost in the predicted distribution is used as a measure.\n",
        "\n",
        "$$KDL(p||q) = \\int_x p(x) \\log \\frac{p(x)}{q(x)} dx$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkyXvdIQZZyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf8caf62-82c8-43da-9c24-57cf1ed86599"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='kl_divergence', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=50, validation_data=(test_images, test_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 17s 5ms/step - loss: 437.4926 - accuracy: 0.0099 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4919 - accuracy: 0.0099 - val_loss: 437.4907 - val_accuracy: 1.0000e-04\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4919 - accuracy: 0.0147 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4923 - accuracy: 0.0073 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4919 - accuracy: 0.0056 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4922 - accuracy: 0.0111 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4922 - accuracy: 0.0099 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4920 - accuracy: 0.0109 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4922 - accuracy: 0.0094 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4924 - accuracy: 0.0108 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 437.4919 - accuracy: 0.0111 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4920 - accuracy: 0.0097 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4921 - accuracy: 0.0110 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4921 - accuracy: 0.0110 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4921 - accuracy: 0.0105 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4920 - accuracy: 0.0098 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4924 - accuracy: 0.0121 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4924 - accuracy: 0.0118 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4924 - accuracy: 0.0077 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4921 - accuracy: 0.0105 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4921 - accuracy: 0.0079 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 437.4919 - accuracy: 0.0117 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4926 - accuracy: 0.0093 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4921 - accuracy: 0.0107 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4921 - accuracy: 0.0101 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4918 - accuracy: 0.0111 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4922 - accuracy: 0.0126 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4919 - accuracy: 0.0103 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4919 - accuracy: 0.0106 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4922 - accuracy: 0.0133 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4920 - accuracy: 0.0096 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4924 - accuracy: 0.0106 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 437.4922 - accuracy: 0.0126 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4920 - accuracy: 0.0094 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4920 - accuracy: 0.0081 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4922 - accuracy: 0.0103 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4922 - accuracy: 0.0119 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4923 - accuracy: 0.0132 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4922 - accuracy: 0.0092 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4922 - accuracy: 0.0089 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4922 - accuracy: 0.0110 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4922 - accuracy: 0.0113 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4919 - accuracy: 0.0102 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4924 - accuracy: 0.0120 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4921 - accuracy: 0.0091 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4922 - accuracy: 0.0126 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4924 - accuracy: 0.0087 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4921 - accuracy: 0.0098 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4920 - accuracy: 0.0093 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4926 - accuracy: 0.0100 - val_loss: 437.4907 - val_accuracy: 0.0500\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5ebc346f10>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIOrqUGTYjD8"
      },
      "source": [
        "##Binary Cross Entropy\n",
        "Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy between two probability distributions, whereas cross-entropy can be thought to calculate the total entropy between the distributions.\n",
        "\n",
        "Cross-entropy is also related to and often confused with logistic loss, called log loss. Although the two measures are derived from a different source, when used as loss functions for classification models, both measures calculate the same quantity and can be used interchangeably.\n",
        "\n",
        "Binary crossentropy is a loss function that is used in binary classification tasks. These are tasks that answer a question with only two choices (yes or no, A or B, 0 or 1, left or right). Several independent such questions can be answered at the same time, as in multi-label classification or in binary image segmentation. Formally, this loss is equal to the average of the categorical crossentropy loss on many two-category tasks.\n",
        "\n",
        "$$BCE = -\\frac{1}{N} \\sum_{i=1}^N y_i \\cdot \\log(p(y_i)) + (1-y_i) \\cdot \\log(1- p(y_i))$$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_labels = tf.reshape(tf.one_hot(training_labels, 100), [training_labels.shape[0], 100])\n",
        "print(training_labels.shape)\n",
        "\n",
        "test_labels = tf.reshape(tf.one_hot(test_labels, 100), [test_labels.shape[0], 100])\n",
        "print(test_labels.shape)"
      ],
      "metadata": {
        "id": "Ckc0jYu_LwkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aa929be-68ad-4904-f604-6d1b41216954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 100)\n",
            "(10000, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRvhNnnJYjOF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31b43384-a782-4dc0-fbe0-ed022b7a79ec"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=50, validation_data=(test_images, test_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0437 - accuracy: 0.1434 - val_loss: 0.0348 - val_accuracy: 0.2194\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0337 - accuracy: 0.2506 - val_loss: 0.0325 - val_accuracy: 0.2778\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0317 - accuracy: 0.3002 - val_loss: 0.0309 - val_accuracy: 0.3202\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0305 - accuracy: 0.3332 - val_loss: 0.0300 - val_accuracy: 0.3443\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0294 - accuracy: 0.3618 - val_loss: 0.0290 - val_accuracy: 0.3740\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0286 - accuracy: 0.3827 - val_loss: 0.0285 - val_accuracy: 0.3815\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0280 - accuracy: 0.3987 - val_loss: 0.0291 - val_accuracy: 0.3633\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0273 - accuracy: 0.4142 - val_loss: 0.0281 - val_accuracy: 0.3962\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0268 - accuracy: 0.4270 - val_loss: 0.0273 - val_accuracy: 0.4153\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0264 - accuracy: 0.4360 - val_loss: 0.0271 - val_accuracy: 0.4196\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0259 - accuracy: 0.4468 - val_loss: 0.0267 - val_accuracy: 0.4284\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0256 - accuracy: 0.4562 - val_loss: 0.0269 - val_accuracy: 0.4247\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0253 - accuracy: 0.4629 - val_loss: 0.0267 - val_accuracy: 0.4293\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0250 - accuracy: 0.4715 - val_loss: 0.0262 - val_accuracy: 0.4443\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0248 - accuracy: 0.4761 - val_loss: 0.0261 - val_accuracy: 0.4485\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0246 - accuracy: 0.4810 - val_loss: 0.0262 - val_accuracy: 0.4432\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0244 - accuracy: 0.4847 - val_loss: 0.0260 - val_accuracy: 0.4477\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0241 - accuracy: 0.4889 - val_loss: 0.0263 - val_accuracy: 0.4417\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0240 - accuracy: 0.4937 - val_loss: 0.0256 - val_accuracy: 0.4613\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0238 - accuracy: 0.5000 - val_loss: 0.0257 - val_accuracy: 0.4541\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0236 - accuracy: 0.5019 - val_loss: 0.0257 - val_accuracy: 0.4590\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0234 - accuracy: 0.5068 - val_loss: 0.0258 - val_accuracy: 0.4592\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0233 - accuracy: 0.5100 - val_loss: 0.0256 - val_accuracy: 0.4635\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0232 - accuracy: 0.5138 - val_loss: 0.0257 - val_accuracy: 0.4624\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0230 - accuracy: 0.5176 - val_loss: 0.0258 - val_accuracy: 0.4596\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0229 - accuracy: 0.5179 - val_loss: 0.0264 - val_accuracy: 0.4439\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0228 - accuracy: 0.5225 - val_loss: 0.0259 - val_accuracy: 0.4569\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0226 - accuracy: 0.5277 - val_loss: 0.0253 - val_accuracy: 0.4733\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0225 - accuracy: 0.5273 - val_loss: 0.0256 - val_accuracy: 0.4655\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0225 - accuracy: 0.5291 - val_loss: 0.0260 - val_accuracy: 0.4595\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0223 - accuracy: 0.5337 - val_loss: 0.0256 - val_accuracy: 0.4681\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0222 - accuracy: 0.5371 - val_loss: 0.0260 - val_accuracy: 0.4610\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0221 - accuracy: 0.5376 - val_loss: 0.0251 - val_accuracy: 0.4798\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0220 - accuracy: 0.5398 - val_loss: 0.0262 - val_accuracy: 0.4549\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0219 - accuracy: 0.5453 - val_loss: 0.0265 - val_accuracy: 0.4551\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0218 - accuracy: 0.5437 - val_loss: 0.0258 - val_accuracy: 0.4700\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0218 - accuracy: 0.5457 - val_loss: 0.0260 - val_accuracy: 0.4602\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0217 - accuracy: 0.5470 - val_loss: 0.0258 - val_accuracy: 0.4598\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0216 - accuracy: 0.5511 - val_loss: 0.0255 - val_accuracy: 0.4701\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0215 - accuracy: 0.5559 - val_loss: 0.0255 - val_accuracy: 0.4688\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0214 - accuracy: 0.5554 - val_loss: 0.0256 - val_accuracy: 0.4692\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0214 - accuracy: 0.5565 - val_loss: 0.0259 - val_accuracy: 0.4682\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0213 - accuracy: 0.5594 - val_loss: 0.0260 - val_accuracy: 0.4678\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0212 - accuracy: 0.5612 - val_loss: 0.0257 - val_accuracy: 0.4671\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0212 - accuracy: 0.5597 - val_loss: 0.0261 - val_accuracy: 0.4617\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0211 - accuracy: 0.5641 - val_loss: 0.0257 - val_accuracy: 0.4684\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0211 - accuracy: 0.5616 - val_loss: 0.0259 - val_accuracy: 0.4692\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0210 - accuracy: 0.5663 - val_loss: 0.0257 - val_accuracy: 0.4687\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0209 - accuracy: 0.5672 - val_loss: 0.0260 - val_accuracy: 0.4664\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0209 - accuracy: 0.5703 - val_loss: 0.0262 - val_accuracy: 0.4579\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5ebc2e19d0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 7\n",
        "\n",
        "Do you see any problems/errors with the above code? Please describe.\n",
        "\n",
        "## Answer 7\n",
        "\n",
        "In the above code, there are 100 classes and we are trying to use binary_crossentropy for the loss. Thus, we must use sparse categorical crossentropy instead. "
      ],
      "metadata": {
        "id": "eAAuecZfTfx2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDBdn-P976v9"
      },
      "source": [
        "## Categorical Cross Entropy\n",
        "\n",
        "This is the most common setting for classification problems. Cross-entropy loss increases as the **predicted probability** strays away from the **actual label**.\n",
        "\n",
        "Note that we have to compare the probabilities (e.g. [0.20, 0.75, 0.05]) of all the classes with the actual labels (e.g., [0, 1, 0]). The actual labels would be one-hot encoding.\n",
        "\n",
        "An important aspect of this is that cross entropy loss penalizes heavily the predictions that are confident but wrong.\n",
        "\n",
        "We are multiplying the log of the actual predicted probability for the ground truth class.\n",
        "\n",
        "$$CCE = -\\frac{1}{N}\\sum_{i=1}^{N}y_i\\log(\\hat{y}_i)$$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uskCvsUSrR0o"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=25, validation_data=(test_images, test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 8\n",
        "\n",
        "Now that you know how CCE works, you need to code it. It should give the same answer as `tf.keras.metrics.categorical_crossentropy` would.\n",
        "\n"
      ],
      "metadata": {
        "id": "KYuLKOGSR4yS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer 8"
      ],
      "metadata": {
        "id": "TAtqr73-SLUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def categorical_crossentropy(true, pred):\n",
        "    \n",
        "    sum = 0\n",
        "    for i in range(true.shape[0]):\n",
        "      sum += tf.reduce_sum(tf.multiply(tf.transpose(true[s]), tf.math.log(pred[s])))\n",
        "      loss = -(1/true.shape[0])*sum\n",
        "      return loss\n",
        "\n",
        "true = tf.constant([[0.0, 1.0, 0.0],\n",
        "                    [1.0, 0.0, 0.0],\n",
        "                    [1.0, 0.0, 0.0],\n",
        "                    [0.0, 0.0, 1.0]])\n",
        "pred = tf.constant([[0.20, 0.70, 0.10],\n",
        "                    [0.80, 0.05, 0.15],\n",
        "                    [0.75, 0.10, 0.15],\n",
        "                    [0.25, 0.15, 0.60]])\n",
        "\n",
        "loss = categorical_crossentropy(true, pred)\n",
        "print(loss)\n",
        "\n",
        "loss = tf.keras.metrics.categorical_crossentropy(true, pred)\n",
        "loss = tf.reduce_mean(loss)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "mxzrT-GKSRSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CHuqni18OPL"
      },
      "source": [
        "## Sparse Categorical Cross Entropy\n",
        "\n",
        "Both, Categorical Cross Entropy [CCE] and Sparse Categorical Cross Entropy [SCCE] have the same loss function. The only difference is the format of $y_i$ (i.e., true labels).\n",
        "\n",
        "If $y_i$'s are one-hot encoded, we should use CCE. Examples (for a 3-class classification): [1,0,0], [0,1,0], [0,0,1]\n",
        "\n",
        "But if $y_i$'s are integers, use SCCE. Examples for above 3-class classification problem: [1], [2], [3]\n",
        "\n",
        "The usage entirely depends on how we load our dataset. One advantage of using sparse categorical cross entropy is it saves time in memory as well as computation because it simply uses a single integer for a class, rather than a whole vector.\n",
        "\n",
        "$$SCCE = -\\log(\\hat{y}_i)$$ for $i$ where $one\\text{-}hot\\text{-}encoding[i] = 1$ "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(training_images, training_labels), (test_images, test_labels) = tf.keras.datasets.cifar100.load_data(label_mode=\"coarse\")\n",
        "\n",
        "training_images=training_images.reshape(50000, 32, 32, 3)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 32, 32, 3)\n",
        "test_images=test_images/255.0"
      ],
      "metadata": {
        "id": "tb6GisE4SkhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcFK26KvCp-g"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=50, validation_data=(test_images, test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhV-fVzcNc3p"
      },
      "source": [
        "## Question 9\n",
        "\n",
        "What is the difference between a Multi-class and Multi-label Classification problem, and what sort of loss function would we need to learn them?\n",
        "\n",
        "## Answer 9\n",
        "\n",
        "Multi-class classification require the loss function from the categorical cross entropy while Multi-label classification requires the binary-class entropy functions. Additionally, Multiclass classification is a classification problem where the task is to classify between more than two classes. Multilabel classification is a classification problem where we get multiple labels as output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITEN1-3-OQwd"
      },
      "source": [
        "## Question 10\n",
        "What is the relationship between Binary Cross entropy and Categorical Cross entropy?\n",
        "\n",
        "## Answer 10\n",
        "\n",
        "Binary cross-entropy is for multi-label classifications, whereas categorical cross entropy is for multi-class classification where each example belongs to a single class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGwk3Z0xOmRk"
      },
      "source": [
        "## Question 11\n",
        "\n",
        "What is the relationship between Sparse Cross entropy and Categorical Cross entropy?\n",
        "\n",
        "## Answer 11\n",
        "\n",
        "Both, categorical cross entropy and sparse categorical cross entropy have the same loss function. The only difference between sparse categorical cross entropy and categorical cross entropy is the format of true labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RDBtUuYjuFh"
      },
      "source": [
        "# **Upload this Day 9 Colab Notebook to your Github repository under \"Day 9\" folder. Also add your *Reflection* on today's learning in README.md**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgg0bvRjS9un"
      },
      "source": [
        "Sources:\n",
        "\n",
        "https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23\n",
        "\n",
        "https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/mean-squared-logarithmic-error-(msle)"
      ]
    }
  ]
}